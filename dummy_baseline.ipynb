{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19658/19658 [00:00<00:00, 190266.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TaxoClass End-to-End Pipeline (Step 1 -> Step 4) + submission.csv\n",
    "# - Keeps the Kaggle submission.csv writing format identical\n",
    "# - Implements:\n",
    "#   Step 1: RoBERTa-large-MNLI entailment similarity sim(D,c)\n",
    "#   Step 2: Core class mining (top-down candidates + median-conf filtering)\n",
    "#   Step 3: Core-guided classifier training (BERT doc encoder + GNN class encoder + matching)\n",
    "#   Step 4: Multi-label self-training (Q refinement + KL objective)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# -------------------------\n",
    "# Paths (given structure)\n",
    "# -------------------------\n",
    "DATA_DIR = \"Amazon_products\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR  = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "TRAIN_CORPUS_PATH = os.path.join(TRAIN_DIR, \"train_corpus.txt\")  # pid \\t text\n",
    "TEST_CORPUS_PATH  = os.path.join(TEST_DIR, \"test_corpus.txt\")    # pid \\t text\n",
    "\n",
    "CLASSES_PATH   = os.path.join(DATA_DIR, \"classes.txt\")\n",
    "HIERARCHY_PATH = os.path.join(DATA_DIR, \"class_hierarchy.txt\")\n",
    "\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "# Kaggle submission constraints (you used these in baseline)\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS  = 1\n",
    "MAX_LABELS  = 3\n",
    "\n",
    "# -------------------------\n",
    "# Config (adjust for speed)\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Step 1 entailment scorer\n",
    "NLI_MODEL_NAME = \"textattack/roberta-base-MNLI\"\n",
    "NLI_TEMPLATE = \"this product is about {}.\"\n",
    "NLI_BATCH_SIZE = 32\n",
    "NLI_MAX_LENGTH = 256\n",
    "NLI_FP16 = True\n",
    "\n",
    "# Step 2 core mining\n",
    "TOPK_PER_PARENT = 1\n",
    "MAX_DEPTH = 4\n",
    "MIN_SIM_TO_EXPAND = 0.2\n",
    "\n",
    "# Step 3 core-guided training\n",
    "DOC_MODEL_NAME = \"bert-base-uncased\"\n",
    "DOC_MAX_LENGTH = 256\n",
    "STEP3_BATCH_SIZE = 4\n",
    "STEP3_EPOCHS = 1\n",
    "STEP3_LR = 2e-5\n",
    "STEP3_NEG_SAMPLE_SIZE = 128\n",
    "GNN_LAYERS = 2\n",
    "EGO_K = 1\n",
    "# Paper form is sigmoid(exp(score)). Practical alternative is sigmoid(score).\n",
    "USE_EXP_IN_MATCHING = False\n",
    "\n",
    "# Step 4 self-training\n",
    "STEP4_BATCH_SIZE = 8\n",
    "STEP4_EPOCHS = 2\n",
    "STEP4_LR = 1e-5\n",
    "Q_UPDATE_EVERY_EPOCH = 1  # recompute class-wise sums each epoch\n",
    "USE_AMP = True\n",
    "\n",
    "# Prediction -> labels\n",
    "PRED_THRESHOLD = None   # e.g., 0.5; if None, use pure topK\n",
    "TOPK_PRED = MAX_LABELS  # cap label count\n",
    "ENSURE_MIN_LABELS = MIN_LABELS\n",
    "\n",
    "# Optional caching (recommended to avoid re-mining/retraining each run)\n",
    "CACHE_DIR = \"taxoclass_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "CORE_CACHE_PATH = os.path.join(CACHE_DIR, \"pid2core.pkl\")\n",
    "MEDIAN_CACHE_PATH = os.path.join(CACHE_DIR, \"median_conf.pkl\")\n",
    "STEP3_MODEL_PATH = os.path.join(CACHE_DIR, \"step3_model.pt\")\n",
    "STEP4_MODEL_PATH = os.path.join(CACHE_DIR, \"step4_model.pt\")\n",
    "\n",
    "# ============================================================\n",
    "# IO helpers\n",
    "# ============================================================\n",
    "def load_corpus(path: str) -> Dict[str, str]:\n",
    "    \"\"\"Load corpus into {pid: text}.\"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "def load_classes(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load class surface names.\n",
    "    Assumption: classes.txt has 531 lines; line index = class_id (0..530).\n",
    "    \"\"\"\n",
    "    classes = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            name = line.strip()\n",
    "            if name:\n",
    "                classes.append(name)\n",
    "    return classes\n",
    "\n",
    "def load_taxonomy_edges(path: str) -> Tuple[Dict[int, List[int]], Dict[int, List[int]]]:\n",
    "    \"\"\"\n",
    "    Load taxonomy parent-child edges.\n",
    "    Expect: each line has two ints: parent child (space or tab separated).\n",
    "    \"\"\"\n",
    "    parent2children = {}\n",
    "    child2parents = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(\"\\t\") if \"\\t\" in line else line.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            p, c = int(parts[0]), int(parts[1])\n",
    "            parent2children.setdefault(p, []).append(c)\n",
    "            child2parents.setdefault(c, []).append(p)\n",
    "    return parent2children, child2parents\n",
    "\n",
    "# ============================================================\n",
    "# Step 1: Document–Class Similarity (NLI entailment)\n",
    "# ============================================================\n",
    "class EntailmentScorer:\n",
    "    \"\"\"\n",
    "    sim(D, c) = P(D entails template(class_name))\n",
    "    Uses roberta-large-mnli.\n",
    "    Caches per pid: {pid: {class_id: sim}}\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"roberta-large-mnli\",\n",
    "        template: str = \"this product is about {}.\",\n",
    "        device: str = \"cuda\",\n",
    "        batch_size: int = 32,\n",
    "        max_length: int = 256,\n",
    "        use_fp16: bool = True,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.template = template\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        self.use_fp16 = use_fp16 and (device == \"cuda\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self._cache: Dict[str, Dict[int, float]] = {}\n",
    "\n",
    "        label2id = {k.upper(): v for k, v in getattr(self.model.config, \"label2id\", {}).items()}\n",
    "        self.entailment_idx = label2id.get(\"ENTAILMENT\", 2)\n",
    "\n",
    "    def _hypothesis(self, class_name: str) -> str:\n",
    "        return self.template.format(class_name)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def score_pid_to_class_ids(\n",
    "        self,\n",
    "        pid: str,\n",
    "        doc_text: str,\n",
    "        class_ids: List[int],\n",
    "        id2name: List[str]\n",
    "    ) -> Dict[int, float]:\n",
    "        if pid not in self._cache:\n",
    "            self._cache[pid] = {}\n",
    "\n",
    "        missing = [cid for cid in class_ids if cid not in self._cache[pid]]\n",
    "        if not missing:\n",
    "            return {cid: self._cache[pid][cid] for cid in class_ids}\n",
    "\n",
    "        for start in range(0, len(missing), self.batch_size):\n",
    "            batch_cids = missing[start:start + self.batch_size]\n",
    "            premises = [doc_text] * len(batch_cids)\n",
    "            hypotheses = [self._hypothesis(id2name[cid]) for cid in batch_cids]\n",
    "\n",
    "            enc = self.tokenizer(\n",
    "                premises, hypotheses,\n",
    "                truncation=True, padding=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            if self.use_fp16:\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    logits = self.model(**enc).logits\n",
    "            else:\n",
    "                logits = self.model(**enc).logits\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)[:, self.entailment_idx].detach().cpu().numpy()\n",
    "            for cid, s in zip(batch_cids, probs.tolist()):\n",
    "                self._cache[pid][cid] = float(s)\n",
    "\n",
    "        return {cid: self._cache[pid][cid] for cid in class_ids}\n",
    "\n",
    "# ============================================================\n",
    "# Taxonomy wrapper (supports pseudo-root)\n",
    "# ============================================================\n",
    "class Taxonomy:\n",
    "    PSEUDO_ROOT = -1\n",
    "\n",
    "    def __init__(self, parent2children: Dict[int, List[int]], child2parents: Dict[int, List[int]], num_classes: int):\n",
    "        self.num_classes = num_classes\n",
    "        self.parent2children = {k: list(v) for k, v in parent2children.items()}\n",
    "        self.child2parents = {k: list(v) for k, v in child2parents.items()}\n",
    "\n",
    "        for cid in range(num_classes):\n",
    "            self.parent2children.setdefault(cid, [])\n",
    "            self.child2parents.setdefault(cid, [])\n",
    "\n",
    "        top_level = [cid for cid in range(num_classes) if len(self.child2parents[cid]) == 0]\n",
    "        self.parent2children[self.PSEUDO_ROOT] = top_level\n",
    "        for cid in top_level:\n",
    "            self.child2parents[cid].append(self.PSEUDO_ROOT)\n",
    "\n",
    "        self._siblings = self._build_siblings()\n",
    "\n",
    "    def parents(self, cid: int) -> List[int]:\n",
    "        return self.child2parents.get(cid, [])\n",
    "\n",
    "    def children(self, cid: int) -> List[int]:\n",
    "        return self.parent2children.get(cid, [])\n",
    "\n",
    "    def siblings(self, cid: int) -> List[int]:\n",
    "        return self._siblings.get(cid, [])\n",
    "\n",
    "    def _build_siblings(self) -> Dict[int, List[int]]:\n",
    "        sib = {cid: set() for cid in range(self.num_classes)}\n",
    "        for cid in range(self.num_classes):\n",
    "            for p in self.parents(cid):\n",
    "                for s in self.children(p):\n",
    "                    if s != cid and s != self.PSEUDO_ROOT and 0 <= s < self.num_classes:\n",
    "                        sib[cid].add(s)\n",
    "        return {cid: sorted(list(v)) for cid, v in sib.items()}\n",
    "\n",
    "# ============================================================\n",
    "# Step 2: Document Core Class Mining\n",
    "# ============================================================\n",
    "class CoreClassMiner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        taxonomy: Taxonomy,\n",
    "        entailment_scorer: EntailmentScorer,\n",
    "        class_names: List[str],\n",
    "        topk_per_parent: int = 2,\n",
    "        max_depth: int = 6,\n",
    "        min_sim_to_expand: float = 0.0,\n",
    "    ):\n",
    "        self.tax = taxonomy\n",
    "        self.scorer = entailment_scorer\n",
    "        self.class_names = class_names\n",
    "        self.topk_per_parent = topk_per_parent\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sim_to_expand = min_sim_to_expand\n",
    "\n",
    "    def select_candidates_for_doc(self, pid: str, doc_text: str) -> Set[int]:\n",
    "        visited: Set[int] = set()\n",
    "        ps: Dict[int, float] = {self.tax.PSEUDO_ROOT: 1.0}\n",
    "\n",
    "        frontier = [self.tax.PSEUDO_ROOT]\n",
    "        depth = 0\n",
    "\n",
    "        while frontier and depth < self.max_depth:\n",
    "            next_frontier = []\n",
    "            depth += 1\n",
    "\n",
    "            for u in frontier:\n",
    "                children = [c for c in self.tax.children(u) if c != self.tax.PSEUDO_ROOT]\n",
    "                if not children:\n",
    "                    continue\n",
    "\n",
    "                sim_map = self.scorer.score_pid_to_class_ids(pid, doc_text, children, self.class_names)\n",
    "\n",
    "                scored = []\n",
    "                for c in children:\n",
    "                    sim_c = sim_map[c]\n",
    "                    if sim_c < self.min_sim_to_expand:\n",
    "                        continue\n",
    "\n",
    "                    best_parent_ps = 0.0\n",
    "                    for p in self.tax.parents(c):\n",
    "                        if p in ps:\n",
    "                            best_parent_ps = max(best_parent_ps, ps[p])\n",
    "                    if best_parent_ps == 0.0:\n",
    "                        best_parent_ps = ps.get(u, 0.0)\n",
    "\n",
    "                    ps_c = best_parent_ps * sim_c\n",
    "                    ps[c] = max(ps.get(c, 0.0), ps_c)\n",
    "                    scored.append((c, ps[c]))\n",
    "\n",
    "                if not scored:\n",
    "                    continue\n",
    "\n",
    "                scored.sort(key=lambda x: x[1], reverse=True)\n",
    "                keep = [c for c, _ in scored[: self.topk_per_parent]]\n",
    "\n",
    "                for c in keep:\n",
    "                    visited.add(c)\n",
    "                    next_frontier.append(c)\n",
    "\n",
    "            frontier = next_frontier\n",
    "\n",
    "        return visited\n",
    "\n",
    "    def _conf_required_class_ids(self, c: int) -> Set[int]:\n",
    "        req = {c}\n",
    "        for p in self.tax.parents(c):\n",
    "            if p != self.tax.PSEUDO_ROOT and 0 <= p < self.tax.num_classes:\n",
    "                req.add(p)\n",
    "        for s in self.tax.siblings(c):\n",
    "            req.add(s)\n",
    "        return req\n",
    "\n",
    "    def compute_conf_for_doc_candidates(self, pid: str, doc_text: str, candidates: Set[int]) -> Dict[int, float]:\n",
    "        if not candidates:\n",
    "            return {}\n",
    "\n",
    "        needed = set()\n",
    "        for c in candidates:\n",
    "            needed |= self._conf_required_class_ids(c)\n",
    "        needed_list = sorted(list(needed))\n",
    "\n",
    "        sim_map = self.scorer.score_pid_to_class_ids(pid, doc_text, needed_list, self.class_names)\n",
    "\n",
    "        conf_map = {}\n",
    "        for c in candidates:\n",
    "            sim_c = sim_map[c]\n",
    "            comp = []\n",
    "            for p in self.tax.parents(c):\n",
    "                if p != self.tax.PSEUDO_ROOT and 0 <= p < self.tax.num_classes:\n",
    "                    comp.append(p)\n",
    "            comp.extend(self.tax.siblings(c))\n",
    "            max_other = max((sim_map[x] for x in comp), default=0.0)\n",
    "            conf_map[c] = float(sim_c - max_other)\n",
    "        return conf_map\n",
    "\n",
    "    def mine_core_classes_over_corpus(\n",
    "        self,\n",
    "        pid2text: Dict[str, str],\n",
    "        show_progress: bool = True,\n",
    "    ) -> Tuple[Dict[str, Set[int]], Dict[int, float]]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          pid2core: {pid: set(core classes)}\n",
    "          median_conf: {class_id: median threshold}\n",
    "        \"\"\"\n",
    "        class_conf_pool: Dict[int, List[float]] = defaultdict(list)\n",
    "        pid2conf: Dict[str, Dict[int, float]] = {}\n",
    "\n",
    "        iterator = tqdm(pid2text.items(), desc=\"Step2-Pass1 (cand/conf)\") if show_progress else pid2text.items()\n",
    "        for pid, text in iterator:\n",
    "            cand = self.select_candidates_for_doc(pid, text)\n",
    "            conf_map = self.compute_conf_for_doc_candidates(pid, text, cand)\n",
    "            pid2conf[pid] = conf_map\n",
    "            for c, v in conf_map.items():\n",
    "                class_conf_pool[c].append(v)\n",
    "\n",
    "        median_conf: Dict[int, float] = {}\n",
    "        for c in range(self.tax.num_classes):\n",
    "            vals = class_conf_pool.get(c, [])\n",
    "            median_conf[c] = float(statistics.median(vals)) if vals else float(\"inf\")\n",
    "\n",
    "        pid2core: Dict[str, Set[int]] = {}\n",
    "        iterator2 = tqdm(pid2conf.items(), desc=\"Step2-Pass2 (core)\") if show_progress else pid2conf.items()\n",
    "        for pid, conf_map in iterator2:\n",
    "            core = {c for c, v in conf_map.items() if v >= median_conf[c]}\n",
    "            pid2core[pid] = core\n",
    "\n",
    "        return pid2core, median_conf\n",
    "\n",
    "# ============================================================\n",
    "# Step 3: Core Class Guided Classifier Training\n",
    "# ============================================================\n",
    "def build_pos_neg_sets_for_doc(core: Set[int], tax: Taxonomy, num_classes: int) -> Tuple[Set[int], Set[int]]:\n",
    "    if not core:\n",
    "        return set(), set()\n",
    "\n",
    "    pos = set(core)\n",
    "    chd_union = set()\n",
    "    for c in core:\n",
    "        for p in tax.parents(c):\n",
    "            if p != tax.PSEUDO_ROOT and 0 <= p < num_classes:\n",
    "                pos.add(p)\n",
    "        for ch in tax.children(c):\n",
    "            if ch != tax.PSEUDO_ROOT and 0 <= ch < num_classes:\n",
    "                chd_union.add(ch)\n",
    "\n",
    "    all_classes = set(range(num_classes))\n",
    "    neg = all_classes - pos - chd_union\n",
    "    return pos, neg\n",
    "\n",
    "class CoreGuidedTrainDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pid2text: Dict[str, str],\n",
    "        pid2core: Dict[str, Set[int]],\n",
    "        tax: Taxonomy,\n",
    "        num_classes: int,\n",
    "        neg_sample_size: Optional[int] = 256,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        self.items = []\n",
    "        rng = random.Random(seed)\n",
    "\n",
    "        for pid, text in pid2text.items():\n",
    "            core = pid2core.get(pid, set())\n",
    "            if not core:\n",
    "                continue\n",
    "\n",
    "            pos, neg = build_pos_neg_sets_for_doc(core, tax, num_classes)\n",
    "\n",
    "            if neg_sample_size is not None and len(neg) > neg_sample_size:\n",
    "                neg = set(rng.sample(sorted(list(neg)), neg_sample_size))\n",
    "\n",
    "            self.items.append((text, pos, neg))\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text, pos, neg = self.items[idx]\n",
    "        y = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        m = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        if pos:\n",
    "            pos_idx = torch.tensor(list(pos), dtype=torch.long)\n",
    "            y[pos_idx] = 1.0\n",
    "            m[pos_idx] = 1.0\n",
    "        if neg:\n",
    "            neg_idx = torch.tensor(list(neg), dtype=torch.long)\n",
    "            y[neg_idx] = 0.0\n",
    "            m[neg_idx] = 1.0\n",
    "        return {\"text\": text, \"y\": y, \"mask\": m}\n",
    "\n",
    "@dataclass\n",
    "class TrainBatch:\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "    mask: torch.Tensor\n",
    "\n",
    "def make_collate_fn(tokenizer, max_length: int = 256):\n",
    "    def collate(examples: List[dict]) -> TrainBatch:\n",
    "        texts = [ex[\"text\"] for ex in examples]\n",
    "        enc = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        y = torch.stack([ex[\"y\"] for ex in examples], dim=0)\n",
    "        m = torch.stack([ex[\"mask\"] for ex in examples], dim=0)\n",
    "        return TrainBatch(enc[\"input_ids\"], enc[\"attention_mask\"], y, m)\n",
    "    return collate\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_dim, out_dim, bias=False)\n",
    "\n",
    "    def forward(self, H: torch.Tensor, A_norm: torch.Tensor) -> torch.Tensor:\n",
    "        HW = self.W(H).float()\n",
    "        A = A_norm.float()\n",
    "        out = torch.sparse.mm(A_norm, HW)\n",
    "        return F.relu(out)\n",
    "\n",
    "def build_normalized_adjacency(\n",
    "    tax: Taxonomy,\n",
    "    num_classes: int,\n",
    "    add_self_loops: bool = True,\n",
    "    undirected: bool = True,\n",
    "    device: str = \"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    edges = set()\n",
    "    for p, children in tax.parent2children.items():\n",
    "        if p == tax.PSEUDO_ROOT:\n",
    "            continue\n",
    "        for c in children:\n",
    "            if c == tax.PSEUDO_ROOT:\n",
    "                continue\n",
    "            if 0 <= p < num_classes and 0 <= c < num_classes:\n",
    "                edges.add((p, c))\n",
    "                if undirected:\n",
    "                    edges.add((c, p))\n",
    "    if add_self_loops:\n",
    "        for i in range(num_classes):\n",
    "            edges.add((i, i))\n",
    "\n",
    "    rows = torch.tensor([u for (u, v) in edges], dtype=torch.long, device=device)\n",
    "    cols = torch.tensor([v for (u, v) in edges], dtype=torch.long, device=device)\n",
    "\n",
    "    deg = torch.zeros(num_classes, dtype=torch.float32, device=device)\n",
    "    deg.scatter_add_(0, rows, torch.ones_like(rows, dtype=torch.float32))\n",
    "    deg = torch.clamp(deg, min=1.0)\n",
    "\n",
    "    vals = 1.0 / torch.sqrt(deg[rows] * deg[cols])\n",
    "\n",
    "    A = torch.sparse_coo_tensor(\n",
    "        indices=torch.stack([rows, cols], dim=0),\n",
    "        values=vals,\n",
    "        size=(num_classes, num_classes),\n",
    "        device=device\n",
    "    ).coalesce()\n",
    "    return A\n",
    "\n",
    "def compute_k_hop_ego_sets(tax: Taxonomy, num_classes: int, k: int = 1) -> List[List[int]]:\n",
    "    adj = [[] for _ in range(num_classes)]\n",
    "    for p, children in tax.parent2children.items():\n",
    "        if p == tax.PSEUDO_ROOT:\n",
    "            continue\n",
    "        if 0 <= p < num_classes:\n",
    "            for c in children:\n",
    "                if 0 <= c < num_classes:\n",
    "                    adj[p].append(c)\n",
    "                    adj[c].append(p)\n",
    "\n",
    "    ego = []\n",
    "    for center in range(num_classes):\n",
    "        seen = {center}\n",
    "        frontier = [center]\n",
    "        for _ in range(k):\n",
    "            nxt = []\n",
    "            for u in frontier:\n",
    "                for v in adj[u]:\n",
    "                    if v not in seen:\n",
    "                        seen.add(v)\n",
    "                        nxt.append(v)\n",
    "            frontier = nxt\n",
    "            if not frontier:\n",
    "                break\n",
    "        ego.append(sorted(list(seen)))\n",
    "    return ego\n",
    "\n",
    "class ClassEncoderGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        bert_tokenizer: AutoTokenizer,\n",
    "        bert_embedding_weight: torch.Tensor,\n",
    "        tax: Taxonomy,\n",
    "        num_classes: int,\n",
    "        hidden_dim: int = 768,\n",
    "        gnn_layers: int = 2,\n",
    "        ego_k: int = 1,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.tokenizer = bert_tokenizer\n",
    "\n",
    "        with torch.no_grad():\n",
    "            H0 = []\n",
    "            for name in class_names:\n",
    "                toks = self.tokenizer(name, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "                if toks.numel() == 0:\n",
    "                    vec = torch.zeros(hidden_dim)\n",
    "                else:\n",
    "                    vec = bert_embedding_weight[toks].mean(dim=0).cpu()\n",
    "                H0.append(vec)\n",
    "            H0 = torch.stack(H0, dim=0)\n",
    "        self.register_buffer(\"H0\", H0.to(device), persistent=False)\n",
    "\n",
    "        self.register_buffer(\"A_norm\", build_normalized_adjacency(tax, num_classes, device=device), persistent=False)\n",
    "        self.ego_sets = compute_k_hop_ego_sets(tax, num_classes, k=ego_k)\n",
    "\n",
    "        self.layers = nn.ModuleList([GCNLayer(hidden_dim, hidden_dim) for _ in range(gnn_layers)])\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        H = self.H0\n",
    "        for layer in self.layers:\n",
    "            H = layer(H, self.A_norm)\n",
    "\n",
    "        C = torch.empty((self.num_classes, self.hidden_dim), device=H.device, dtype=H.dtype)\n",
    "        for j in range(self.num_classes):\n",
    "            nodes = self.ego_sets[j]\n",
    "            C[j] = H[nodes].mean(dim=0)\n",
    "        return C\n",
    "\n",
    "class TaxoClassModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Single model used for Step 3 and Step 4.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        class_names: List[str],\n",
    "        tax: Taxonomy,\n",
    "        doc_model_name: str = \"bert-base-uncased\",\n",
    "        gnn_layers: int = 2,\n",
    "        ego_k: int = 1,\n",
    "        use_exp_in_matching: bool = False,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.use_exp_in_matching = use_exp_in_matching\n",
    "        self.device = device\n",
    "\n",
    "        self.doc_tokenizer = AutoTokenizer.from_pretrained(doc_model_name, use_fast=True)\n",
    "        self.doc_encoder = AutoModel.from_pretrained(doc_model_name).to(device)\n",
    "\n",
    "        hidden_dim = self.doc_encoder.config.hidden_size\n",
    "        self.B = nn.Parameter(torch.empty(hidden_dim, hidden_dim))\n",
    "        nn.init.xavier_uniform_(self.B)\n",
    "\n",
    "        bert_emb = self.doc_encoder.get_input_embeddings().weight.detach()\n",
    "        self.class_encoder = ClassEncoderGNN(\n",
    "            class_names=class_names,\n",
    "            bert_tokenizer=self.doc_tokenizer,\n",
    "            bert_embedding_weight=bert_emb,\n",
    "            tax=tax,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=hidden_dim,\n",
    "            gnn_layers=gnn_layers,\n",
    "            ego_k=ego_k,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def encode_docs(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.doc_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return out.last_hidden_state[:, 0, :]\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        D = self.encode_docs(input_ids, attention_mask)  # [B,dim] (AMP면 fp16일 수 있음)\n",
    "\n",
    "        # GNN(class encoder)만 FP32로 수행 (sparse.mm Half 방지)\n",
    "        if self.device == \"cuda\":\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                C = self.class_encoder()  # [N,dim] float32\n",
    "        else:\n",
    "            C = self.class_encoder()\n",
    "\n",
    "        # matmul dtype 정합: 안전하게 float32로 계산 후 sigmoid\n",
    "        Df = D.float()\n",
    "        Bf = self.B.float()\n",
    "        scores = Df @ (C @ Bf).t()\n",
    "\n",
    "        if self.use_exp_in_matching:\n",
    "            return torch.sigmoid(torch.exp(scores))\n",
    "        return torch.sigmoid(scores)\n",
    "\n",
    "\n",
    "def core_guided_loss(p: torch.Tensor, y: torch.Tensor, mask: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    p = torch.clamp(p, eps, 1.0 - eps)\n",
    "    per_label = y * torch.log(p) + (1.0 - y) * torch.log(1.0 - p)\n",
    "    per_label = per_label * mask\n",
    "    denom = torch.clamp(mask.sum(dim=1), min=1.0)\n",
    "    loss_per_doc = -per_label.sum(dim=1) / denom\n",
    "    return loss_per_doc.mean()\n",
    "\n",
    "def train_step3(\n",
    "    pid2text_train: Dict[str, str],\n",
    "    pid2core: Dict[str, Set[int]],\n",
    "    class_names: List[str],\n",
    "    tax: Taxonomy,\n",
    "    device: str,\n",
    ") -> TaxoClassModel:\n",
    "    model = TaxoClassModel(\n",
    "        num_classes=len(class_names),\n",
    "        class_names=class_names,\n",
    "        tax=tax,\n",
    "        doc_model_name=DOC_MODEL_NAME,\n",
    "        gnn_layers=GNN_LAYERS,\n",
    "        ego_k=EGO_K,\n",
    "        use_exp_in_matching=USE_EXP_IN_MATCHING,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "\n",
    "    dataset = CoreGuidedTrainDataset(\n",
    "        pid2text=pid2text_train,\n",
    "        pid2core=pid2core,\n",
    "        tax=tax,\n",
    "        num_classes=len(class_names),\n",
    "        neg_sample_size=STEP3_NEG_SAMPLE_SIZE,\n",
    "        seed=42,\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=STEP3_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=make_collate_fn(model.doc_tokenizer, max_length=DOC_MAX_LENGTH),\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=STEP3_LR, weight_decay=0.01)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device == \"cuda\"))\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(1, STEP3_EPOCHS + 1):\n",
    "        pbar = tqdm(loader, desc=f\"Step3 (epoch {ep}/{STEP3_EPOCHS})\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch.input_ids.to(device)\n",
    "            attention_mask = batch.attention_mask.to(device)\n",
    "            y = batch.y.to(device)\n",
    "            m = batch.mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler.is_enabled():\n",
    "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                    p = model(input_ids, attention_mask)\n",
    "                    loss = core_guided_loss(p, y, m)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                p = model(input_ids, attention_mask)\n",
    "                loss = core_guided_loss(p, y, m)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(loss=float(loss.detach().cpu()))\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# Step 4: Multi-label Self-Training\n",
    "# ============================================================\n",
    "class UnlabeledTextDataset(Dataset):\n",
    "    def __init__(self, pid2text: Dict[str, str]):\n",
    "        self.pids = list(pid2text.keys())\n",
    "        self.texts = [pid2text[pid] for pid in self.pids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return {\"pid\": self.pids[idx], \"text\": self.texts[idx]}\n",
    "\n",
    "@dataclass\n",
    "class UnlabeledBatch:\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "\n",
    "def make_unlabeled_collate_fn(tokenizer, max_length: int = 256):\n",
    "    def collate(examples: List[dict]) -> UnlabeledBatch:\n",
    "        texts = [ex[\"text\"] for ex in examples]\n",
    "        enc = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        return UnlabeledBatch(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "    return collate\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_classwise_sums(model: TaxoClassModel, loader: DataLoader, device: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "      sum_p[j]     = sum_i p_ij\n",
    "      sum_1mp[j]   = sum_i (1 - p_ij)\n",
    "    Needed by Q refinement formula.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sum_p = None\n",
    "    sum_1mp = None\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Step4-Q update (sums)\", leave=False):\n",
    "        input_ids = batch.input_ids.to(device)\n",
    "        attention_mask = batch.attention_mask.to(device)\n",
    "        p = model(input_ids, attention_mask)  # [B,C]\n",
    "        if sum_p is None:\n",
    "            sum_p = p.sum(dim=0)\n",
    "            sum_1mp = (1.0 - p).sum(dim=0)\n",
    "        else:\n",
    "            sum_p += p.sum(dim=0)\n",
    "            sum_1mp += (1.0 - p).sum(dim=0)\n",
    "\n",
    "    # avoid division by zero\n",
    "    sum_p = torch.clamp(sum_p, min=1e-8)\n",
    "    sum_1mp = torch.clamp(sum_1mp, min=1e-8)\n",
    "    return sum_p.detach(), sum_1mp.detach()\n",
    "\n",
    "def refine_q(p: torch.Tensor, sum_p: torch.Tensor, sum_1mp: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Paper formula:\n",
    "      q_ij = (p_ij^2 / sum_i p_ij) / ((p_ij^2 / sum_i p_ij) + ((1-p_ij)^2 / sum_i (1-p_ij)))\n",
    "    p: [B,C], sum_p: [C], sum_1mp: [C]\n",
    "    \"\"\"\n",
    "    p = torch.clamp(p, eps, 1.0 - eps)\n",
    "    a = (p * p) / sum_p\n",
    "    b = ((1.0 - p) * (1.0 - p)) / sum_1mp\n",
    "    q = a / (a + b + eps)\n",
    "    return torch.clamp(q, eps, 1.0 - eps)\n",
    "\n",
    "def bernoulli_kl(q: torch.Tensor, p: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    KL(Bern(q) || Bern(p)) = q log(q/p) + (1-q) log((1-q)/(1-p))\n",
    "    \"\"\"\n",
    "    q = torch.clamp(q, eps, 1.0 - eps)\n",
    "    p = torch.clamp(p, eps, 1.0 - eps)\n",
    "    return (q * torch.log(q / p) + (1.0 - q) * torch.log((1.0 - q) / (1.0 - p))).mean()\n",
    "\n",
    "def self_train_step4(\n",
    "    model: TaxoClassModel,\n",
    "    pid2text_unlabeled: Dict[str, str],\n",
    "    device: str,\n",
    ") -> TaxoClassModel:\n",
    "    dataset = UnlabeledTextDataset(pid2text_unlabeled)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=STEP4_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=make_unlabeled_collate_fn(model.doc_tokenizer, max_length=DOC_MAX_LENGTH),\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=STEP4_LR, weight_decay=0.01)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and device == \"cuda\"))\n",
    "\n",
    "    sum_p, sum_1mp = None, None\n",
    "\n",
    "    for ep in range(1, STEP4_EPOCHS + 1):\n",
    "        if sum_p is None or (Q_UPDATE_EVERY_EPOCH and ((ep - 1) % Q_UPDATE_EVERY_EPOCH == 0)):\n",
    "            # recompute class-wise sums on current model\n",
    "            eval_loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=STEP4_BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                collate_fn=make_unlabeled_collate_fn(model.doc_tokenizer, max_length=DOC_MAX_LENGTH),\n",
    "            )\n",
    "            sum_p, sum_1mp = compute_classwise_sums(model, eval_loader, device)\n",
    "\n",
    "        model.train()\n",
    "        pbar = tqdm(loader, desc=f\"Step4 (epoch {ep}/{STEP4_EPOCHS})\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch.input_ids.to(device)\n",
    "            attention_mask = batch.attention_mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                    p = model(input_ids, attention_mask)\n",
    "                    q = refine_q(p, sum_p.to(device), sum_1mp.to(device))\n",
    "                    loss = bernoulli_kl(q, p)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                p = model(input_ids, attention_mask)\n",
    "                q = refine_q(p, sum_p.to(device), sum_1mp.to(device))\n",
    "                loss = bernoulli_kl(q, p)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(loss=float(loss.detach().cpu()))\n",
    "\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# Inference + label selection\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def predict_proba(\n",
    "    model: TaxoClassModel,\n",
    "    pid2text: Dict[str, str],\n",
    "    device: str,\n",
    "    batch_size: int = 16,\n",
    "    max_length: int = 256,\n",
    ") -> Tuple[List[str], np.ndarray]:\n",
    "    model.eval()\n",
    "    pids = list(pid2text.keys())\n",
    "    texts = [pid2text[pid] for pid in pids]\n",
    "\n",
    "    all_probs = []\n",
    "    for start in tqdm(range(0, len(texts), batch_size), desc=\"Predicting\"):\n",
    "        batch_texts = texts[start:start + batch_size]\n",
    "        enc = model.doc_tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        p = model(enc[\"input_ids\"], enc[\"attention_mask\"]).detach().cpu().numpy()\n",
    "        all_probs.append(p)\n",
    "\n",
    "    probs = np.vstack(all_probs) if all_probs else np.zeros((0, model.num_classes), dtype=np.float32)\n",
    "    return pids, probs\n",
    "\n",
    "def probs_to_labels(\n",
    "    probs_row: np.ndarray,\n",
    "    topk: int = 3,\n",
    "    threshold: Optional[float] = None,\n",
    "    ensure_min: int = 1\n",
    ") -> List[int]:\n",
    "    idx_sorted = np.argsort(-probs_row)  # descending\n",
    "    if threshold is not None:\n",
    "        chosen = [int(i) for i in idx_sorted if probs_row[i] >= threshold]\n",
    "        chosen = chosen[:topk] if topk is not None else chosen\n",
    "        if len(chosen) < ensure_min:\n",
    "            chosen = [int(i) for i in idx_sorted[:ensure_min]]\n",
    "        return sorted(chosen)\n",
    "    else:\n",
    "        chosen = [int(i) for i in idx_sorted[:max(ensure_min, topk)]]\n",
    "        return sorted(chosen)\n",
    "\n",
    "# ============================================================\n",
    "# Main pipeline\n",
    "# ============================================================\n",
    "def main():\n",
    "    # Load data\n",
    "    pid2text_train = load_corpus(TRAIN_CORPUS_PATH)\n",
    "    pid2text_test  = load_corpus(TEST_CORPUS_PATH)\n",
    "    class_names = load_classes(CLASSES_PATH)\n",
    "    assert len(class_names) == NUM_CLASSES, f\"Expected {NUM_CLASSES} classes, got {len(class_names)}\"\n",
    "\n",
    "    parent2children, child2parents = load_taxonomy_edges(HIERARCHY_PATH)\n",
    "    tax = Taxonomy(parent2children, child2parents, num_classes=len(class_names))\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 2: Core mining (uses Step 1 entailment scorer)\n",
    "    # -------------------------\n",
    "    if os.path.exists(CORE_CACHE_PATH) and os.path.exists(MEDIAN_CACHE_PATH):\n",
    "        with open(CORE_CACHE_PATH, \"rb\") as f:\n",
    "            pid2core = pickle.load(f)\n",
    "        with open(MEDIAN_CACHE_PATH, \"rb\") as f:\n",
    "            median_conf = pickle.load(f)\n",
    "        print(f\"[Cache] Loaded pid2core ({len(pid2core)}) and median_conf from {CACHE_DIR}\")\n",
    "    else:\n",
    "        entail_scorer = EntailmentScorer(\n",
    "            model_name=NLI_MODEL_NAME,\n",
    "            template=NLI_TEMPLATE,\n",
    "            device=DEVICE,\n",
    "            batch_size=NLI_BATCH_SIZE,\n",
    "            max_length=NLI_MAX_LENGTH,\n",
    "            use_fp16=NLI_FP16,\n",
    "        )\n",
    "        miner = CoreClassMiner(\n",
    "            taxonomy=tax,\n",
    "            entailment_scorer=entail_scorer,\n",
    "            class_names=class_names,\n",
    "            topk_per_parent=TOPK_PER_PARENT,\n",
    "            max_depth=MAX_DEPTH,\n",
    "            min_sim_to_expand=MIN_SIM_TO_EXPAND,\n",
    "        )\n",
    "        pid2core, median_conf = miner.mine_core_classes_over_corpus(pid2text_train, show_progress=True)\n",
    "\n",
    "        with open(CORE_CACHE_PATH, \"wb\") as f:\n",
    "            pickle.dump(pid2core, f)\n",
    "        with open(MEDIAN_CACHE_PATH, \"wb\") as f:\n",
    "            pickle.dump(median_conf, f)\n",
    "        print(f\"[Cache] Saved pid2core and median_conf to {CACHE_DIR}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 3: Core-guided training\n",
    "    # -------------------------\n",
    "    if os.path.exists(STEP3_MODEL_PATH):\n",
    "        step3_model = TaxoClassModel(\n",
    "            num_classes=len(class_names),\n",
    "            class_names=class_names,\n",
    "            tax=tax,\n",
    "            doc_model_name=DOC_MODEL_NAME,\n",
    "            gnn_layers=GNN_LAYERS,\n",
    "            ego_k=EGO_K,\n",
    "            use_exp_in_matching=USE_EXP_IN_MATCHING,\n",
    "            device=DEVICE,\n",
    "        ).to(DEVICE)\n",
    "        step3_model.load_state_dict(torch.load(STEP3_MODEL_PATH, map_location=DEVICE))\n",
    "        print(f\"[Cache] Loaded Step 3 model from {STEP3_MODEL_PATH}\")\n",
    "    else:\n",
    "        step3_model = train_step3(\n",
    "            pid2text_train=pid2text_train,\n",
    "            pid2core=pid2core,\n",
    "            class_names=class_names,\n",
    "            tax=tax,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        torch.save(step3_model.state_dict(), STEP3_MODEL_PATH)\n",
    "        print(f\"[Cache] Saved Step 3 model to {STEP3_MODEL_PATH}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 4: Multi-label self-training\n",
    "    # -------------------------\n",
    "    if os.path.exists(STEP4_MODEL_PATH):\n",
    "        step4_model = TaxoClassModel(\n",
    "            num_classes=len(class_names),\n",
    "            class_names=class_names,\n",
    "            tax=tax,\n",
    "            doc_model_name=DOC_MODEL_NAME,\n",
    "            gnn_layers=GNN_LAYERS,\n",
    "            ego_k=EGO_K,\n",
    "            use_exp_in_matching=USE_EXP_IN_MATCHING,\n",
    "            device=DEVICE,\n",
    "        ).to(DEVICE)\n",
    "        step4_model.load_state_dict(torch.load(STEP4_MODEL_PATH, map_location=DEVICE))\n",
    "        print(f\"[Cache] Loaded Step 4 model from {STEP4_MODEL_PATH}\")\n",
    "    else:\n",
    "        step4_model = self_train_step4(step3_model, pid2text_train, device=DEVICE)\n",
    "        torch.save(step4_model.state_dict(), STEP4_MODEL_PATH)\n",
    "        print(f\"[Cache] Saved Step 4 model to {STEP4_MODEL_PATH}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Predict on test set\n",
    "    # -------------------------\n",
    "    pid_list_test, probs_test = predict_proba(\n",
    "        step4_model,\n",
    "        pid2text_test,\n",
    "        device=DEVICE,\n",
    "        batch_size=16,\n",
    "        max_length=DOC_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    # Convert probs -> label lists\n",
    "    all_pids, all_labels = [], []\n",
    "    for pid, prob_row in zip(pid_list_test, probs_test):\n",
    "        labels = probs_to_labels(\n",
    "            prob_row,\n",
    "            topk=TOPK_PRED,\n",
    "            threshold=PRED_THRESHOLD,\n",
    "            ensure_min=ENSURE_MIN_LABELS\n",
    "        )\n",
    "        all_pids.append(pid)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # ============================================================\n",
    "    # --- Save submission file ---\n",
    "    # (Same structure/format as your baseline; only labels source changed.)\n",
    "    # ============================================================\n",
    "    with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"labels\"])\n",
    "        for pid, labels in zip(all_pids, all_labels):\n",
    "            writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "    print(f\"Submission file saved to: {SUBMISSION_PATH}\")\n",
    "    print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
