{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19658/19658 [00:00<00:00, 190266.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TaxoClass-style HMTC pipeline (3.2 / 3.3 / 3.4 reflected)\n",
    "# - 3.2.1: path score based top-down candidate selection\n",
    "# - 3.2.2: conf vs parent/sibling + median saliency + multi-core set\n",
    "# - 3.3: GNN class encoder + (log-)bilinear matching + pos/neg masked BCE\n",
    "# - 3.4: multi-label self-training with hierarchy closure + masked loss\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_DIR = \"Amazon_products\"\n",
    "CLASSES_PATH   = os.path.join(BASE_DIR, \"classes.txt\")\n",
    "HIER_PATH      = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "KEYWORDS_PATH  = os.path.join(BASE_DIR, \"class_related_keywords.txt\")\n",
    "TRAIN_PATH     = os.path.join(BASE_DIR, \"train\", \"train_corpus.txt\")\n",
    "TEST_PATH      = os.path.join(BASE_DIR, \"test\", \"test_corpus.txt\")\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS = 2\n",
    "MAX_LABELS = 3\n",
    "\n",
    "# -------- Stage-1: Bi-encoder (fast retrieval) ----------\n",
    "BI_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BI_BATCH = 256\n",
    "BI_MAX_TOK_DOC = 256\n",
    "BI_MAX_TOK_CLASS = 64\n",
    "\n",
    "# -------- Stage-2: NLI (expensive similarity) ----------\n",
    "NLI_MODEL_NAME = \"textattack/roberta-base-MNLI\"\n",
    "NLI_BATCH = 32\n",
    "NLI_MAX_LEN = 256\n",
    "NLI_USE_FP16 = True\n",
    "MAX_CHARS_DOC_FOR_NLI = 600\n",
    "\n",
    "# -------- 3.2 core mining params (reflect paper) ----------\n",
    "TOPK_CHILDREN_BI = 50       # bi로 자식 후보를 넓게 줄임 (속도/품질 trade-off)\n",
    "MAX_LEVELS = 12             # taxonomy 깊이에 따라 조정 가능\n",
    "MAX_CORES_PER_DOC = 5       # multi-core set 상한 (너무 많아지는 것 방지)\n",
    "MIN_SIM_ABS = 0.55          # sim 자체가 너무 낮으면 core 후보에서 제외 (실무 안정화용)\n",
    "\n",
    "# paper: at level l, for each node choose (l+2) children; and keep (l+1)^2 nodes.\n",
    "# 아래 구현은 이 원칙을 따르되, NLI 호출은 bi로 줄인 후보에만 수행.\n",
    "\n",
    "# -------- 3.3 classifier ----------\n",
    "DOC_MODEL_NAME = \"bert-base-uncased\"  # paper uses BERT-base\n",
    "DOC_MAX_TOK = 256\n",
    "CLASS_NAME_MAX_TOK = 32              # class surface name encoding\n",
    "TRAIN_EPOCHS = 2\n",
    "LR = 2e-5\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "# Matching function\n",
    "MATCH_USE_EXP = False  # True로 하면 sigmoid(exp(bilinear)) 형태(권장X: 확률이 0.5 이상으로 쏠릴 수 있음)\n",
    "\n",
    "# -------- 3.4 self-training ----------\n",
    "SELF_TRAIN_EPOCHS = 1\n",
    "PSEUDO_POS_TAU = 0.90        # 높은 confidence만 추가\n",
    "PSEUDO_MIN_POS = 1\n",
    "\n",
    "# HF controls\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)\n",
    "HF_CACHE_DIR = os.getenv(\"HF_HOME\", None)\n",
    "HF_LOCAL_ONLY = False        # 인터넷 OFF면 True\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Utils: IO + taxonomy parsing\n",
    "# ============================================================\n",
    "def load_corpus(path: str):\n",
    "    pids, texts = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pids.append(pid)\n",
    "                texts.append(text)\n",
    "    return pids, texts\n",
    "\n",
    "def load_classes(path: str):\n",
    "    id2name = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            parts = re.split(r\"[\\t,]\", s, maxsplit=1)\n",
    "            if len(parts) == 2 and parts[0].strip().isdigit():\n",
    "                cid = int(parts[0].strip())\n",
    "                id2name[cid] = parts[1].strip()\n",
    "            else:\n",
    "                id2name[i] = s\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        if cid not in id2name:\n",
    "            id2name[cid] = f\"class_{cid}\"\n",
    "    return id2name\n",
    "\n",
    "def load_keywords_accumulate(path: str):\n",
    "    kw = {cid: [] for cid in range(NUM_CLASSES)}\n",
    "\n",
    "    def split_tokens(fields):\n",
    "        out = []\n",
    "        for r in fields:\n",
    "            out.extend([x.strip() for x in re.split(r\"[,;/|]\", r) if x.strip()])\n",
    "        out = [x for x in out if not re.fullmatch(r\"[-+]?\\d+(\\.\\d+)?\", x)]\n",
    "        return out\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            parts = s.split(\"\\t\")\n",
    "            if len(parts) == 1:\n",
    "                parts = s.split(\",\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            cid_raw = parts[0].strip()\n",
    "            if not cid_raw.isdigit():\n",
    "                continue\n",
    "            cid = int(cid_raw)\n",
    "            if cid < 0 or cid >= NUM_CLASSES:\n",
    "                continue\n",
    "            rest = [p.strip() for p in parts[1:] if p.strip()]\n",
    "            toks = split_tokens(rest)\n",
    "\n",
    "            seen = set(kw[cid])\n",
    "            for t in toks:\n",
    "                if t not in seen:\n",
    "                    kw[cid].append(t)\n",
    "                    seen.add(t)\n",
    "    return kw\n",
    "\n",
    "def read_edges(path: str, id2name: Dict[int, str]):\n",
    "    name2id = {v: k for k, v in id2name.items()}\n",
    "\n",
    "    def to_id(x):\n",
    "        x = x.strip()\n",
    "        if x.isdigit():\n",
    "            return int(x)\n",
    "        return name2id.get(x, None)\n",
    "\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            parts = re.split(r\"[\\t,]\", s)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            a = to_id(parts[0]); b = to_id(parts[1])\n",
    "            if a is None or b is None or a == b:\n",
    "                continue\n",
    "            if 0 <= a < NUM_CLASSES and 0 <= b < NUM_CLASSES:\n",
    "                edges.append((a, b))\n",
    "    return edges\n",
    "\n",
    "def build_graph(edges, parent_to_child=True):\n",
    "    parents_of = {i: set() for i in range(NUM_CLASSES)}\n",
    "    children_of = {i: set() for i in range(NUM_CLASSES)}\n",
    "    for a, b in edges:\n",
    "        if parent_to_child:\n",
    "            p, c = a, b\n",
    "        else:\n",
    "            p, c = b, a\n",
    "        if p != c:\n",
    "            parents_of[c].add(p)\n",
    "            children_of[p].add(c)\n",
    "\n",
    "    roots = [i for i in range(NUM_CLASSES) if len(parents_of[i]) == 0]\n",
    "    parents_of = {k: sorted(v) for k, v in parents_of.items()}\n",
    "    children_of = {k: sorted(v) for k, v in children_of.items()}\n",
    "    return roots, parents_of, children_of\n",
    "\n",
    "def load_hierarchy_autodetect(path: str, id2name: Dict[int, str]):\n",
    "    edges = read_edges(path, id2name)\n",
    "    r1, p1, c1 = build_graph(edges, parent_to_child=True)\n",
    "    r2, p2, c2 = build_graph(edges, parent_to_child=False)\n",
    "    if 1 <= len(r2) < len(r1):\n",
    "        return r2, p2, c2, \"inverted(child->parent in file)\"\n",
    "    return r1, p1, c1, \"as-is(parent->child in file)\"\n",
    "\n",
    "def compute_depths(roots, children_of):\n",
    "    depth = np.full(NUM_CLASSES, 10**9, dtype=np.int32)\n",
    "    from collections import deque\n",
    "    q = deque()\n",
    "    for r in roots:\n",
    "        depth[r] = 0\n",
    "        q.append(r)\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        for v in children_of.get(u, []):\n",
    "            if depth[v] > depth[u] + 1:\n",
    "                depth[v] = depth[u] + 1\n",
    "                q.append(v)\n",
    "    maxd = int(np.max(depth[depth < 10**9])) if np.any(depth < 10**9) else 0\n",
    "    depth[depth >= 10**9] = maxd + 1\n",
    "    return depth\n",
    "\n",
    "def build_siblings(parents_of: Dict[int, List[int]], children_of: Dict[int, List[int]]) -> Dict[int, List[int]]:\n",
    "    sib = {i: set() for i in range(NUM_CLASSES)}\n",
    "    for c in range(NUM_CLASSES):\n",
    "        for p in parents_of.get(c, []):\n",
    "            for s in children_of.get(p, []):\n",
    "                if s != c:\n",
    "                    sib[c].add(s)\n",
    "    return {k: sorted(v) for k, v in sib.items()}\n",
    "\n",
    "def get_ancestors_all(cid: int, parents_of: Dict[int, List[int]], max_steps: int = 50) -> List[int]:\n",
    "    # DAG 대비: 첫 부모만 타지 않고, 모든 부모를 확장(단, 폭이 커질 수 있으니 max_steps로 안전장치)\n",
    "    out = []\n",
    "    seen = set()\n",
    "    frontier = [cid]\n",
    "    steps = 0\n",
    "    while frontier and steps < max_steps:\n",
    "        nxt = []\n",
    "        for x in frontier:\n",
    "            for p in parents_of.get(x, []):\n",
    "                if p not in seen:\n",
    "                    seen.add(p)\n",
    "                    out.append(p)\n",
    "                    nxt.append(p)\n",
    "        frontier = nxt\n",
    "        steps += 1\n",
    "    return out\n",
    "\n",
    "def ensure_k_labels(primary: List[int], parents_of) -> List[int]:\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for x in primary:\n",
    "        if x is None:\n",
    "            continue\n",
    "        if x not in seen:\n",
    "            out.append(x); seen.add(x)\n",
    "        if len(out) >= MAX_LABELS:\n",
    "            break\n",
    "\n",
    "    i = 0\n",
    "    while len(out) < MIN_LABELS and i < len(out):\n",
    "        for a in get_ancestors_all(out[i], parents_of, max_steps=10):\n",
    "            if a not in seen:\n",
    "                out.append(a); seen.add(a)\n",
    "            if len(out) >= MIN_LABELS:\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    if len(out) < MIN_LABELS:\n",
    "        for k in range(NUM_CLASSES):\n",
    "            if k not in seen:\n",
    "                out.append(k); seen.add(k)\n",
    "            if len(out) >= MIN_LABELS:\n",
    "                break\n",
    "\n",
    "    return sorted(out[:MAX_LABELS])\n",
    "\n",
    "def build_class_texts(id2name, kw, parents_of):\n",
    "    # retrieval/entailment에 쓸 “설명 문장” 생성(기존과 유사)\n",
    "    texts = []\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        name = id2name[cid]\n",
    "        path_ids = list(reversed(get_ancestors_all(cid, parents_of, max_steps=10))) + [cid]\n",
    "        path_names = [id2name[i] for i in path_ids if i in id2name]\n",
    "        path_str = \" > \".join(path_names)\n",
    "\n",
    "        kws = kw.get(cid, [])[:25]\n",
    "        if kws:\n",
    "            t = f\"Category path: {path_str}. Keywords: \" + \", \".join(kws) + \".\"\n",
    "        else:\n",
    "            t = f\"Category path: {path_str}.\"\n",
    "        texts.append(t)\n",
    "    return texts\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Stage-1: Bi-encoder\n",
    "# ============================================================\n",
    "class BiEncoder:\n",
    "    def __init__(self, model_name: str, device: str, batch_size: int,\n",
    "                 token: Optional[str] = None, cache_dir: Optional[str] = None, local_files_only: bool = False):\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "        except ImportError as e:\n",
    "            raise RuntimeError(\"pip install -q sentence-transformers\") from e\n",
    "\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        kwargs = {}\n",
    "        if cache_dir is not None:\n",
    "            kwargs[\"cache_folder\"] = cache_dir\n",
    "        if local_files_only:\n",
    "            kwargs[\"local_files_only\"] = True\n",
    "        if token is not None:\n",
    "            kwargs[\"token\"] = token\n",
    "\n",
    "        self.model = SentenceTransformer(model_name, device=device, **kwargs)\n",
    "\n",
    "    def encode(self, texts: List[str], batch_size: Optional[int] = None) -> np.ndarray:\n",
    "        bs = batch_size or self.batch_size\n",
    "        emb = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=bs,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        return emb.astype(np.float32)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Stage-2: NLI scorer (sim)\n",
    "# ============================================================\n",
    "class EntailmentScorer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        device: str = \"cuda\",\n",
    "        use_fp16: bool = True,\n",
    "        max_chars_doc: int = 600,\n",
    "        token: Optional[str] = None,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        local_files_only: bool = False,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        try:\n",
    "            import torch\n",
    "            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        except ImportError as e:\n",
    "            raise RuntimeError(\"pip install -q transformers torch\") from e\n",
    "\n",
    "        self.torch = torch\n",
    "        self.device = device\n",
    "        self.max_chars_doc = int(max_chars_doc)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.use_fp16 = bool(use_fp16 and device.startswith(\"cuda\") and torch.cuda.is_available())\n",
    "\n",
    "        common_kwargs = {\n",
    "            \"token\": token,\n",
    "            \"cache_dir\": cache_dir,\n",
    "            \"local_files_only\": local_files_only,\n",
    "        }\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[EntailmentScorer] loading: {model_name}\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, **common_kwargs)\n",
    "        model_kwargs = dict(common_kwargs)\n",
    "        if self.use_fp16:\n",
    "            model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, **model_kwargs)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        cfg = self.model.config\n",
    "        entail_id = None\n",
    "        if hasattr(cfg, \"label2id\") and isinstance(cfg.label2id, dict):\n",
    "            for k, v in cfg.label2id.items():\n",
    "                if \"entail\" in str(k).lower():\n",
    "                    entail_id = int(v)\n",
    "                    break\n",
    "        if entail_id is None:\n",
    "            entail_id = 2\n",
    "        self.entail_id = entail_id\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[EntailmentScorer] entail_id={self.entail_id} | fp16={self.use_fp16}\")\n",
    "\n",
    "    def _encode_batch(self, premise: str, hyps: list, max_len: int):\n",
    "        return self.tokenizer(\n",
    "            [premise] * len(hyps),\n",
    "            hyps,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def score(self, premise: str, hypotheses: list, batch_size: int = 16, max_len: int = 256) -> np.ndarray:\n",
    "        import numpy as np\n",
    "        torch = self.torch\n",
    "\n",
    "        if not hypotheses:\n",
    "            return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "        premise = (premise or \"\")[: self.max_chars_doc]\n",
    "        bs = int(max(1, batch_size))\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(hypotheses):\n",
    "            cur = hypotheses[i:i+bs]\n",
    "            try:\n",
    "                enc = self._encode_batch(premise, cur, max_len=max_len)\n",
    "                enc = {k: v.to(self.device, non_blocking=True) for k, v in enc.items()}\n",
    "\n",
    "                if self.use_fp16:\n",
    "                    with torch.inference_mode():\n",
    "                        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                            logits = self.model(**enc).logits\n",
    "                            probs = torch.softmax(logits, dim=-1)[:, self.entail_id]\n",
    "                else:\n",
    "                    with torch.inference_mode():\n",
    "                        logits = self.model(**enc).logits\n",
    "                        probs = torch.softmax(logits, dim=-1)[:, self.entail_id]\n",
    "\n",
    "                out.append(probs.detach().float().cpu().numpy())\n",
    "                i += bs\n",
    "            except RuntimeError as e:\n",
    "                msg = str(e).lower()\n",
    "                if (\"out of memory\" in msg or \"cuda\" in msg) and bs > 1 and self.device.startswith(\"cuda\"):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    bs = max(1, bs // 2)\n",
    "                    continue\n",
    "                raise\n",
    "\n",
    "        return np.concatenate(out, axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3.2: Core class mining\n",
    "# - 3.2.1 path score candidate selection (top-down)\n",
    "# - 3.2.2 conf vs parent/sibling + median saliency + multi-core set\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class DocMiningResult:\n",
    "    cand_ids: List[int]                 # C_i^{cand}\n",
    "    sim_cache: Dict[int, float]         # sim(D, c) for scored classes\n",
    "    conf_map: Dict[int, float]          # conf(D, c) for candidate classes (computed after scoring parent/sib)\n",
    "    core_ids: List[int]                 # final multi-core set C_i (after median threshold; filled later)\n",
    "\n",
    "def _hyp(cid: int, id2name: Dict[int, str]) -> str:\n",
    "    return f\"This document is about {id2name[cid]}.\"\n",
    "\n",
    "def _topk_by_bi(doc_emb: np.ndarray, cand_ids: List[int], class_embs: np.ndarray, k: int) -> List[int]:\n",
    "    if not cand_ids:\n",
    "        return []\n",
    "    c = np.array(cand_ids, dtype=np.int32)\n",
    "    sim = class_embs[c] @ doc_emb\n",
    "    k_eff = min(k, sim.shape[0])\n",
    "    idx = np.argpartition(-sim, k_eff-1)[:k_eff]\n",
    "    idx = idx[np.argsort(-sim[idx])]\n",
    "    return c[idx].tolist()\n",
    "\n",
    "def core_candidate_selection_path_score(\n",
    "    doc_text: str,\n",
    "    doc_emb: np.ndarray,\n",
    "    class_embs: np.ndarray,\n",
    "    nli: EntailmentScorer,\n",
    "    roots: List[int],\n",
    "    children_of: Dict[int, List[int]],\n",
    "    id2name: Dict[int, str],\n",
    "    max_levels: int = 12,\n",
    ") -> Tuple[List[int], Dict[int, float]]:\n",
    "    \"\"\"\n",
    "    3.2.1: path score(ps) 기반 top-down 후보 선택\n",
    "      - ps(root)=1 (여기서는 virtual root를 두고 roots를 children으로 취급)\n",
    "      - level l에서 큐(선택된 노드들)의 각 노드에 대해:\n",
    "          * 자식 중 (l+2)개를 sim 기준으로 선택\n",
    "      - 다음 레벨 후보 중 ps 기준 상위 (l+1)^2개 유지\n",
    "    반환:\n",
    "      - candidate ids (visited nodes, excluding virtual root)\n",
    "      - sim_cache: sim(D,c) 계산된 값 (NLI 기반)\n",
    "    \"\"\"\n",
    "    sim_cache: Dict[int, float] = {}\n",
    "    ps: Dict[int, float] = {}  # path score\n",
    "    visited: Set[int] = set()\n",
    "\n",
    "    # virtual root handling\n",
    "    current = list(roots)\n",
    "    # level 0: treat roots as children of root, choose 2 (since l=0 => (l+2)=2)\n",
    "    l = 0\n",
    "    need = min(2, len(current))\n",
    "    if need == 0:\n",
    "        return [], sim_cache\n",
    "\n",
    "    # score roots (bi shortlist then NLI)\n",
    "    root_bi = _topk_by_bi(doc_emb, current, class_embs, k=min(TOPK_CHILDREN_BI, len(current)))\n",
    "    hyps = [_hyp(c, id2name) for c in root_bi]\n",
    "    scores = nli.score(doc_text[:MAX_CHARS_DOC_FOR_NLI], hyps, batch_size=NLI_BATCH, max_len=NLI_MAX_LEN)\n",
    "    for c, s in zip(root_bi, scores.tolist()):\n",
    "        sim_cache[c] = float(s)\n",
    "\n",
    "    root_sorted = sorted(root_bi, key=lambda c: sim_cache[c], reverse=True)[:need]\n",
    "    # ps for selected roots: ps(root)=1 so ps(c)=sim(D,c)\n",
    "    for c in root_sorted:\n",
    "        ps[c] = max(ps.get(c, 0.0), sim_cache[c])\n",
    "        visited.add(c)\n",
    "\n",
    "    frontier = root_sorted[:]  # selected at level 0\n",
    "\n",
    "    # levels 0..max_levels-1 expand to level+1\n",
    "    for l in range(0, max_levels):\n",
    "        if not frontier:\n",
    "            break\n",
    "\n",
    "        next_candidates: Set[int] = set()\n",
    "        # For each node in frontier, choose (l+2) best children by sim\n",
    "        for p in frontier:\n",
    "            children = children_of.get(p, [])\n",
    "            if not children:\n",
    "                continue\n",
    "\n",
    "            # bi shortlist\n",
    "            bi_sel = _topk_by_bi(doc_emb, children, class_embs, k=min(TOPK_CHILDREN_BI, len(children)))\n",
    "            if not bi_sel:\n",
    "                continue\n",
    "\n",
    "            # NLI score\n",
    "            need_children = min(l + 2, len(bi_sel))\n",
    "            hyps = [_hyp(c, id2name) for c in bi_sel]\n",
    "            scores = nli.score(doc_text[:MAX_CHARS_DOC_FOR_NLI], hyps, batch_size=NLI_BATCH, max_len=NLI_MAX_LEN)\n",
    "            for c, s in zip(bi_sel, scores.tolist()):\n",
    "                sim_cache[c] = float(s)\n",
    "\n",
    "            best_children = sorted(bi_sel, key=lambda c: sim_cache[c], reverse=True)[:need_children]\n",
    "\n",
    "            # update ps(child) = max_{parent} ps(parent) * sim(child)\n",
    "            for c in best_children:\n",
    "                next_candidates.add(c)\n",
    "                new_ps = ps.get(p, 0.0) * sim_cache.get(c, 0.0)\n",
    "                if new_ps > ps.get(c, 0.0):\n",
    "                    ps[c] = new_ps\n",
    "                visited.add(c)\n",
    "\n",
    "        if not next_candidates:\n",
    "            break\n",
    "\n",
    "        # Keep top (l+1)^2 by path score among next_candidates\n",
    "        keep = min((l + 1) ** 2, len(next_candidates))\n",
    "        frontier = sorted(list(next_candidates), key=lambda c: ps.get(c, 0.0), reverse=True)[:keep]\n",
    "\n",
    "    cand_ids = sorted(list(visited))\n",
    "    return cand_ids, sim_cache\n",
    "\n",
    "def compute_conf_for_doc_candidates(\n",
    "    doc_text: str,\n",
    "    cand_ids: List[int],\n",
    "    sim_cache: Dict[int, float],\n",
    "    nli: EntailmentScorer,\n",
    "    id2name: Dict[int, str],\n",
    "    parents_of: Dict[int, List[int]],\n",
    "    siblings_of: Dict[int, List[int]],\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    3.2.2의 conf 정의:\n",
    "      conf(D,c) = sim(D,c) - max_{c' in Par(c) ∪ Sib(c)} sim(D,c')\n",
    "    부모/형제 sim이 없으면 NLI로 추가 계산(캐시 채움).\n",
    "    \"\"\"\n",
    "    conf_map: Dict[int, float] = {}\n",
    "\n",
    "    # collect missing sims to score in one batch per doc (for efficiency)\n",
    "    to_score: List[int] = []\n",
    "    need_set: Set[int] = set()\n",
    "\n",
    "    for c in cand_ids:\n",
    "        need_set.add(c)\n",
    "        for p in parents_of.get(c, []):\n",
    "            need_set.add(p)\n",
    "        for s in siblings_of.get(c, []):\n",
    "            need_set.add(s)\n",
    "\n",
    "    for x in need_set:\n",
    "        if x not in sim_cache:\n",
    "            to_score.append(x)\n",
    "\n",
    "    if to_score:\n",
    "        hyps = [_hyp(x, id2name) for x in to_score]\n",
    "        scores = nli.score(doc_text[:MAX_CHARS_DOC_FOR_NLI], hyps, batch_size=NLI_BATCH, max_len=NLI_MAX_LEN)\n",
    "        for x, s in zip(to_score, scores.tolist()):\n",
    "            sim_cache[x] = float(s)\n",
    "\n",
    "    for c in cand_ids:\n",
    "        sim_c = sim_cache.get(c, 0.0)\n",
    "        # optional: 너무 낮은 sim은 core 후보에서 제외(노이즈 완화)\n",
    "        if sim_c < MIN_SIM_ABS:\n",
    "            continue\n",
    "\n",
    "        comps = []\n",
    "        comps.extend(parents_of.get(c, []))\n",
    "        comps.extend(siblings_of.get(c, []))\n",
    "        if comps:\n",
    "            max_comp = max(sim_cache.get(x, 0.0) for x in comps)\n",
    "        else:\n",
    "            max_comp = 0.0\n",
    "        conf_map[c] = float(sim_c - max_comp)\n",
    "\n",
    "    return conf_map\n",
    "\n",
    "def mine_cores_with_median_saliency(\n",
    "    texts: List[str],\n",
    "    doc_embs: np.ndarray,\n",
    "    class_embs: np.ndarray,\n",
    "    nli: EntailmentScorer,\n",
    "    roots: List[int],\n",
    "    parents_of: Dict[int, List[int]],\n",
    "    children_of: Dict[int, List[int]],\n",
    "    siblings_of: Dict[int, List[int]],\n",
    "    id2name: Dict[int, str],\n",
    ") -> Tuple[List[DocMiningResult], np.ndarray]:\n",
    "    \"\"\"\n",
    "    전체 코퍼스에 대해:\n",
    "      1) 3.2.1: 후보 cand_ids + sim_cache 산출\n",
    "      2) 3.2.2: 각 문서에서 cand에 대해 conf 계산\n",
    "      3) class-wise median(conf) 계산\n",
    "      4) conf >= median(conf[class]) AND conf>0 인 cand들을 core로 채택 (multi-core)\n",
    "    반환:\n",
    "      - results: 문서별 mining 결과(코어 포함)\n",
    "      - salient_mask: core_ids가 비어있지 않은 문서만 True\n",
    "    \"\"\"\n",
    "    results: List[DocMiningResult] = []\n",
    "    per_class_confs: List[List[float]] = [[] for _ in range(NUM_CLASSES)]\n",
    "\n",
    "    print(\"[STEP2] 3.2 core mining: path score candidates -> conf -> class-wise median -> multi-core\")\n",
    "    for i, (t, e) in enumerate(tqdm(list(zip(texts, doc_embs)), desc=\"3.2 mining\")):\n",
    "        cand_ids, sim_cache = core_candidate_selection_path_score(\n",
    "            doc_text=t,\n",
    "            doc_emb=e,\n",
    "            class_embs=class_embs,\n",
    "            nli=nli,\n",
    "            roots=roots,\n",
    "            children_of=children_of,\n",
    "            id2name=id2name,\n",
    "            max_levels=MAX_LEVELS,\n",
    "        )\n",
    "        conf_map = compute_conf_for_doc_candidates(\n",
    "            doc_text=t,\n",
    "            cand_ids=cand_ids,\n",
    "            sim_cache=sim_cache,\n",
    "            nli=nli,\n",
    "            id2name=id2name,\n",
    "            parents_of=parents_of,\n",
    "            siblings_of=siblings_of,\n",
    "        )\n",
    "        for c, v in conf_map.items():\n",
    "            if 0 <= c < NUM_CLASSES:\n",
    "                per_class_confs[c].append(float(v))\n",
    "\n",
    "        results.append(DocMiningResult(\n",
    "            cand_ids=cand_ids,\n",
    "            sim_cache=sim_cache,\n",
    "            conf_map=conf_map,\n",
    "            core_ids=[],\n",
    "        ))\n",
    "\n",
    "    # class-wise median(conf)\n",
    "    class_median = np.full(NUM_CLASSES, np.inf, dtype=np.float32)\n",
    "    for c in range(NUM_CLASSES):\n",
    "        if per_class_confs[c]:\n",
    "            class_median[c] = float(np.median(np.array(per_class_confs[c], dtype=np.float32)))\n",
    "\n",
    "    salient = np.zeros(len(results), dtype=bool)\n",
    "\n",
    "    # decide multi-core set per doc\n",
    "    for i, r in enumerate(results):\n",
    "        core = []\n",
    "        for c, conf in r.conf_map.items():\n",
    "            if conf <= 0.0:\n",
    "                continue\n",
    "            med = class_median[c]\n",
    "            if np.isfinite(med) and conf >= med:\n",
    "                core.append(c)\n",
    "\n",
    "        # sort by conf descending, cap\n",
    "        core = sorted(core, key=lambda c: r.conf_map.get(c, -1e9), reverse=True)[:MAX_CORES_PER_DOC]\n",
    "        r.core_ids = core\n",
    "        salient[i] = (len(core) > 0)\n",
    "\n",
    "    print(f\"[STEP2] salient docs (non-empty multi-core): {int(salient.sum())} / {len(salient)}\")\n",
    "    return results, salient\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3.3: Core-guided classifier training\n",
    "# - GNN class encoder\n",
    "# - bilinear matching (matrix B)\n",
    "# - pos/neg set design + masked BCE\n",
    "# ============================================================\n",
    "def build_pos_neg_sets_for_doc(\n",
    "    core_ids: List[int],\n",
    "    parents_of: Dict[int, List[int]],\n",
    "    children_of: Dict[int, List[int]],\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Paper (Eq.7):\n",
    "      C_pos = core ∪ parents(core)\n",
    "      C_neg = C \\ C_pos \\ children(core)\n",
    "    \"\"\"\n",
    "    pos: Set[int] = set()\n",
    "    chd: Set[int] = set()\n",
    "\n",
    "    for c in core_ids:\n",
    "        pos.add(c)\n",
    "        for p in get_ancestors_all(c, parents_of, max_steps=20):\n",
    "            pos.add(p)\n",
    "        for k in children_of.get(c, []):\n",
    "            chd.add(k)\n",
    "\n",
    "    # negative: all - pos - chd\n",
    "    neg = [i for i in range(NUM_CLASSES) if (i not in pos and i not in chd)]\n",
    "    return sorted(list(pos)), neg\n",
    "\n",
    "def train_core_guided_classifier(\n",
    "    train_texts: List[str],\n",
    "    mining_results: List[DocMiningResult],\n",
    "    salient_mask: np.ndarray,\n",
    "    id2name: Dict[int, str],\n",
    "    parents_of: Dict[int, List[int]],\n",
    "    children_of: Dict[int, List[int]],\n",
    "    device: str,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    batch_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    3.3 implementation:\n",
    "      - doc encoder: BERT-base CLS\n",
    "      - class encoder: GNN over taxonomy, init from class surface name embeddings\n",
    "      - matcher: bilinear with matrix B (+ bias)\n",
    "      - loss: masked BCE on pos/neg sets only\n",
    "      - skip docs where core set empty (salient_mask)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        from torch.utils.data import Dataset, DataLoader\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"pip install -q transformers torch\") from e\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # Build training examples (only salient docs)\n",
    "    idx = np.where(salient_mask)[0].tolist()\n",
    "    if not idx:\n",
    "        raise RuntimeError(\"No salient docs found. Consider relaxing thresholds (MIN_SIM_ABS / MAX_LEVELS / PSEUDO_POS_TAU).\")\n",
    "\n",
    "    ex_texts = [train_texts[i] for i in idx]\n",
    "    ex_cores = [mining_results[i].core_ids for i in idx]\n",
    "\n",
    "    # Build pos/neg lists for each example\n",
    "    ex_pos_neg = []\n",
    "    for core_ids in ex_cores:\n",
    "        pos_ids, neg_ids = build_pos_neg_sets_for_doc(core_ids, parents_of, children_of)\n",
    "        ex_pos_neg.append((pos_ids, neg_ids))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DOC_MODEL_NAME, token=HF_TOKEN, cache_dir=HF_CACHE_DIR, local_files_only=HF_LOCAL_ONLY)\n",
    "    doc_encoder = AutoModel.from_pretrained(DOC_MODEL_NAME, token=HF_TOKEN, cache_dir=HF_CACHE_DIR, local_files_only=HF_LOCAL_ONLY).to(device)\n",
    "\n",
    "    # ---- class initial features: encode surface names with same transformer (once) ----\n",
    "    class_names = [id2name[i] for i in range(NUM_CLASSES)]\n",
    "    with torch.no_grad():\n",
    "        doc_encoder.eval()\n",
    "        feats = []\n",
    "        for i in range(0, NUM_CLASSES, 64):\n",
    "            batch = class_names[i:i+64]\n",
    "            enc = tokenizer(batch, truncation=True, padding=True, max_length=CLASS_NAME_MAX_TOK, return_tensors=\"pt\").to(device)\n",
    "            out = doc_encoder(**enc).last_hidden_state[:, 0, :]  # CLS\n",
    "            feats.append(out.detach())\n",
    "        class_init = torch.cat(feats, dim=0)  # (C, d)\n",
    "        d_model = class_init.size(-1)\n",
    "\n",
    "    # ---- build sparse adjacency (undirected + self-loop), row-normalized mean ----\n",
    "    def build_sparse_row_mean(edges: List[Tuple[int, int]], n: int, device: str):\n",
    "        # edges list is directed; for undirected, add both ways outside\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for a, b in edges:\n",
    "            rows.append(a); cols.append(b)\n",
    "        # self loops\n",
    "        for i in range(n):\n",
    "            rows.append(i); cols.append(i)\n",
    "        idx = torch.tensor([rows, cols], dtype=torch.long, device=device)\n",
    "        vals = torch.ones(idx.size(1), dtype=torch.float32, device=device)\n",
    "        A = torch.sparse_coo_tensor(idx, vals, size=(n, n)).coalesce()\n",
    "        deg = torch.sparse.sum(A, dim=1).to_dense().clamp(min=1.0)\n",
    "        # row-mean: D^{-1} A\n",
    "        inv_deg = 1.0 / deg\n",
    "        # apply row scaling by multiplying values with inv_deg[row]\n",
    "        r = A.indices()[0]\n",
    "        v = A.values() * inv_deg[r]\n",
    "        A_mean = torch.sparse_coo_tensor(A.indices(), v, size=A.size(), device=device).coalesce()\n",
    "        return A_mean\n",
    "\n",
    "    # taxonomy edges: parent<->child\n",
    "    undirected_edges = []\n",
    "    for p, chs in children_of.items():\n",
    "        for c in chs:\n",
    "            undirected_edges.append((p, c))\n",
    "            undirected_edges.append((c, p))\n",
    "\n",
    "    # ego pooling matrix E: each row i averages over {i} ∪ parents(i) ∪ children(i)\n",
    "    def build_sparse_ego_pool(parents_of, children_of, n: int, device: str):\n",
    "        rows, cols = [], []\n",
    "        for i in range(n):\n",
    "            ego = set([i])\n",
    "            ego.update(parents_of.get(i, []))\n",
    "            ego.update(children_of.get(i, []))\n",
    "            ego = sorted(list(ego))\n",
    "            for j in ego:\n",
    "                rows.append(i); cols.append(j)\n",
    "        idx = torch.tensor([rows, cols], dtype=torch.long, device=device)\n",
    "        vals = torch.ones(idx.size(1), dtype=torch.float32, device=device)\n",
    "        E = torch.sparse_coo_tensor(idx, vals, size=(n, n)).coalesce()\n",
    "        deg = torch.sparse.sum(E, dim=1).to_dense().clamp(min=1.0)\n",
    "        inv_deg = 1.0 / deg\n",
    "        r = E.indices()[0]\n",
    "        v = E.values() * inv_deg[r]\n",
    "        E_mean = torch.sparse_coo_tensor(E.indices(), v, size=E.size(), device=device).coalesce()\n",
    "        return E_mean\n",
    "\n",
    "    A_mean = build_sparse_row_mean(undirected_edges, NUM_CLASSES, device)\n",
    "    E_mean = build_sparse_ego_pool(parents_of, children_of, NUM_CLASSES, device)\n",
    "\n",
    "    class ClassGNNEncoder(nn.Module):\n",
    "        def __init__(self, init_feats: torch.Tensor, A_mean, E_mean, hidden_dim: int):\n",
    "            super().__init__()\n",
    "            self.register_buffer(\"h0\", init_feats)  # fixed init\n",
    "            self.A = A_mean\n",
    "            self.E = E_mean\n",
    "            self.fc1 = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "            self.fc2 = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "            self.act = nn.ReLU()\n",
    "\n",
    "        def forward(self):\n",
    "            # simple 2-layer mean-GCN style\n",
    "            h = self.h0\n",
    "            h = self.act(self.fc1(h + torch.sparse.mm(self.A, h)))\n",
    "            h = self.act(self.fc2(h + torch.sparse.mm(self.A, h)))\n",
    "            # ego pooling (paper's ego network representation)\n",
    "            c = torch.sparse.mm(self.E, h)\n",
    "            return c  # (C, d)\n",
    "\n",
    "    class CoreGuidedMatcher(nn.Module):\n",
    "        def __init__(self, doc_encoder, class_encoder, d_model: int):\n",
    "            super().__init__()\n",
    "            self.doc_encoder = doc_encoder\n",
    "            self.class_encoder = class_encoder\n",
    "            self.B = nn.Parameter(torch.eye(d_model))         # bilinear matrix\n",
    "            self.bias = nn.Parameter(torch.zeros(NUM_CLASSES))\n",
    "            self.use_exp = MATCH_USE_EXP\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            doc = self.doc_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]  # (B, d)\n",
    "            cls = self.class_encoder()  # (C, d)\n",
    "            # bilinear: (doc @ B) @ cls^T\n",
    "            proj = doc @ self.B\n",
    "            logits = proj @ cls.t() + self.bias\n",
    "            if self.use_exp:\n",
    "                logits = torch.exp(logits)  # optional: mimic σ(exp(.)) formulation\n",
    "            return logits\n",
    "\n",
    "    class PseudoDataset(Dataset):\n",
    "        def __init__(self, texts: List[str], pos_neg: List[Tuple[List[int], List[int]]]):\n",
    "            self.texts = texts\n",
    "            self.pos_neg = pos_neg\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            t = self.texts[idx]\n",
    "            pos_ids, neg_ids = self.pos_neg[idx]\n",
    "            return t, np.array(pos_ids, dtype=np.int64), np.array(neg_ids, dtype=np.int64)\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [b[0] for b in batch]\n",
    "        pos = [b[1] for b in batch]\n",
    "        neg = [b[2] for b in batch]\n",
    "        # build dense target/mask per batch only\n",
    "        B = len(batch)\n",
    "        y = np.zeros((B, NUM_CLASSES), dtype=np.float32)\n",
    "        m = np.zeros((B, NUM_CLASSES), dtype=np.float32)\n",
    "        for i in range(B):\n",
    "            if pos[i].size > 0:\n",
    "                y[i, pos[i]] = 1.0\n",
    "                m[i, pos[i]] = 1.0\n",
    "            if neg[i].size > 0:\n",
    "                # y already 0\n",
    "                m[i, neg[i]] = 1.0\n",
    "        return texts, torch.tensor(y), torch.tensor(m)\n",
    "\n",
    "    ds = PseudoDataset(ex_texts, ex_pos_neg)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=False, collate_fn=collate)\n",
    "\n",
    "    class_encoder = ClassGNNEncoder(class_init, A_mean, E_mean, hidden_dim=d_model).to(device)\n",
    "    model = CoreGuidedMatcher(doc_encoder, class_encoder, d_model=d_model).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    bce = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def masked_bce_loss(logits, y, mask):\n",
    "        # logits,y,mask: (B,C)\n",
    "        loss = bce(logits, y) * mask\n",
    "        denom = mask.sum().clamp(min=1.0)\n",
    "        return loss.sum() / denom\n",
    "\n",
    "    print(f\"[STEP3] training core-guided matcher | samples={len(ds)} | device={device}\")\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for texts, y, mask in tqdm(dl, desc=f\"train ep{ep+1}/{epochs}\"):\n",
    "            enc = tokenizer(list(texts), truncation=True, padding=True, max_length=DOC_MAX_TOK, return_tensors=\"pt\").to(device)\n",
    "            y = y.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            logits = model(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "            loss = masked_bce_loss(logits, y, mask)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            total += float(loss.detach().cpu())\n",
    "        avg = total / max(1, len(dl))\n",
    "        print(f\"[STEP3] epoch {ep+1}: loss={avg:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Prediction helpers + 3.4 self-training\n",
    "# ============================================================\n",
    "def predict_probs(model, tokenizer, texts: List[str], batch_size: int, device: str) -> np.ndarray:\n",
    "    import torch\n",
    "    probs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"predict\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, truncation=True, padding=True, max_length=DOC_MAX_TOK, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "            logits = model(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "            p = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        probs.append(p)\n",
    "    return np.vstack(probs)\n",
    "\n",
    "def self_training_build_examples(\n",
    "    texts: List[str],\n",
    "    probs: np.ndarray,\n",
    "    parents_of: Dict[int, List[int]],\n",
    "    children_of: Dict[int, List[int]],\n",
    "    tau: float,\n",
    ") -> Tuple[List[str], List[Tuple[List[int], List[int]]]]:\n",
    "    \"\"\"\n",
    "    3.4 multi-label self-training:\n",
    "      - p >= tau 인 라벨을 positive로 선택\n",
    "      - ancestor closure 적용\n",
    "      - negative는 paper처럼: all - pos - children(pos)\n",
    "    \"\"\"\n",
    "    ex_texts = []\n",
    "    ex_pos_neg = []\n",
    "    for t, p in zip(texts, probs):\n",
    "        pos = np.where(p >= tau)[0].tolist()\n",
    "        if len(pos) < PSEUDO_MIN_POS:\n",
    "            pos = [int(np.argmax(p))]\n",
    "\n",
    "        # ancestor closure\n",
    "        pos_set = set(pos)\n",
    "        for c in list(pos_set):\n",
    "            for a in get_ancestors_all(c, parents_of, max_steps=20):\n",
    "                pos_set.add(a)\n",
    "\n",
    "        # children 제외 negative\n",
    "        chd = set()\n",
    "        for c in list(pos_set):\n",
    "            for k in children_of.get(c, []):\n",
    "                chd.add(k)\n",
    "\n",
    "        neg = [i for i in range(NUM_CLASSES) if (i not in pos_set and i not in chd)]\n",
    "        ex_texts.append(t)\n",
    "        ex_pos_neg.append((sorted(list(pos_set)), neg))\n",
    "    return ex_texts, ex_pos_neg\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "def main():\n",
    "    for p in [CLASSES_PATH, HIER_PATH, KEYWORDS_PATH, TRAIN_PATH, TEST_PATH]:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    except Exception:\n",
    "        device = \"cpu\"\n",
    "    print(f\"[INFO] device: {device}\")\n",
    "\n",
    "    id2name = load_classes(CLASSES_PATH)\n",
    "    kw = load_keywords_accumulate(KEYWORDS_PATH)\n",
    "    roots, parents_of, children_of, note = load_hierarchy_autodetect(HIER_PATH, id2name)\n",
    "    siblings_of = build_siblings(parents_of, children_of)\n",
    "    depth_arr = compute_depths(roots, children_of)\n",
    "    print(f\"[INFO] taxonomy direction: {note} | roots={len(roots)} | max_depth≈{int(depth_arr.max())}\")\n",
    "\n",
    "    train_pids, train_texts = load_corpus(TRAIN_PATH)\n",
    "    test_pids, test_texts = load_corpus(TEST_PATH)\n",
    "\n",
    "    # Step 1: bi-encoder embeddings (docs + classes)\n",
    "    class_texts = build_class_texts(id2name, kw, parents_of)\n",
    "    bi = BiEncoder(\n",
    "        model_name=BI_MODEL_NAME,\n",
    "        device=device,\n",
    "        batch_size=BI_BATCH if device == \"cuda\" else min(BI_BATCH, 128),\n",
    "        token=HF_TOKEN,\n",
    "        cache_dir=HF_CACHE_DIR,\n",
    "        local_files_only=HF_LOCAL_ONLY,\n",
    "    )\n",
    "    print(\"[STEP1] Encoding class texts (bi-encoder)...\")\n",
    "    class_embs = bi.encode(class_texts, batch_size=min(BI_BATCH, 256))  # (C, D)\n",
    "\n",
    "    print(\"[STEP1] Encoding documents (bi-encoder)...\")\n",
    "    all_texts = train_texts + test_texts\n",
    "    doc_embs = bi.encode(all_texts)  # (N, D)\n",
    "\n",
    "    # Step 2: NLI scorer + 3.2 mining\n",
    "    nli = EntailmentScorer(\n",
    "        model_name=NLI_MODEL_NAME,\n",
    "        device=device,\n",
    "        use_fp16=NLI_USE_FP16,\n",
    "        max_chars_doc=MAX_CHARS_DOC_FOR_NLI,\n",
    "        token=HF_TOKEN,\n",
    "        cache_dir=HF_CACHE_DIR,\n",
    "        local_files_only=HF_LOCAL_ONLY,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    mining_all, salient_all = mine_cores_with_median_saliency(\n",
    "        texts=all_texts,\n",
    "        doc_embs=doc_embs,\n",
    "        class_embs=class_embs,\n",
    "        nli=nli,\n",
    "        roots=roots,\n",
    "        parents_of=parents_of,\n",
    "        children_of=children_of,\n",
    "        siblings_of=siblings_of,\n",
    "        id2name=id2name,\n",
    "    )\n",
    "\n",
    "    n_train = len(train_texts)\n",
    "    mining_train = mining_all[:n_train]\n",
    "    salient_train = salient_all[:n_train]\n",
    "\n",
    "    # Step 3: 3.3 core-guided training (masked BCE)\n",
    "    model, tok = train_core_guided_classifier(\n",
    "        train_texts=train_texts,\n",
    "        mining_results=mining_train,\n",
    "        salient_mask=salient_train,\n",
    "        id2name=id2name,\n",
    "        parents_of=parents_of,\n",
    "        children_of=children_of,\n",
    "        device=device,\n",
    "        epochs=TRAIN_EPOCHS,\n",
    "        lr=LR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # Step 4: 3.4 self-training (transductive: train+test)\n",
    "    all_probs = predict_probs(model, tok, all_texts, batch_size=BATCH_SIZE, device=device)\n",
    "    st_texts, st_pos_neg = self_training_build_examples(\n",
    "        texts=all_texts,\n",
    "        probs=all_probs,\n",
    "        parents_of=parents_of,\n",
    "        children_of=children_of,\n",
    "        tau=PSEUDO_POS_TAU,\n",
    "    )\n",
    "\n",
    "    # retrain with (salient train) + (self-train pseudo)\n",
    "    # 간단히: self-train 전체를 다시 학습 데이터로 사용 (현업에서는 중복/가중치 설계 가능)\n",
    "    # 여기서는 3.3과 동일한 마스크 손실로 1 epoch만 추가 학습.\n",
    "    # (재학습을 위해 mining 결과가 아닌 pos/neg 리스트를 직접 사용)\n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        from torch.utils.data import Dataset, DataLoader\n",
    "        from transformers import AutoTokenizer\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"pip install -q transformers torch\") from e\n",
    "\n",
    "    class PseudoDataset2(Dataset):\n",
    "        def __init__(self, texts: List[str], pos_neg: List[Tuple[List[int], List[int]]]):\n",
    "            self.texts = texts\n",
    "            self.pos_neg = pos_neg\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            t = self.texts[idx]\n",
    "            pos, neg = self.pos_neg[idx]\n",
    "            return t, np.array(pos, dtype=np.int64), np.array(neg, dtype=np.int64)\n",
    "\n",
    "    def collate2(batch):\n",
    "        texts = [b[0] for b in batch]\n",
    "        pos = [b[1] for b in batch]\n",
    "        neg = [b[2] for b in batch]\n",
    "        B = len(batch)\n",
    "        y = np.zeros((B, NUM_CLASSES), dtype=np.float32)\n",
    "        m = np.zeros((B, NUM_CLASSES), dtype=np.float32)\n",
    "        for i in range(B):\n",
    "            if pos[i].size > 0:\n",
    "                y[i, pos[i]] = 1.0\n",
    "                m[i, pos[i]] = 1.0\n",
    "            if neg[i].size > 0:\n",
    "                m[i, neg[i]] = 1.0\n",
    "        return texts, torch.tensor(y), torch.tensor(m)\n",
    "\n",
    "    # reuse model/tok, continue training\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    bce = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def masked_bce_loss(logits, y, mask):\n",
    "        loss = bce(logits, y) * mask\n",
    "        denom = mask.sum().clamp(min=1.0)\n",
    "        return loss.sum() / denom\n",
    "\n",
    "    ds2 = PseudoDataset2(st_texts, st_pos_neg)\n",
    "    dl2 = DataLoader(ds2, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, collate_fn=collate2)\n",
    "\n",
    "    print(f\"[STEP4] self-training fine-tune | samples={len(ds2)} | epochs={SELF_TRAIN_EPOCHS}\")\n",
    "    model.train()\n",
    "    for ep in range(SELF_TRAIN_EPOCHS):\n",
    "        total = 0.0\n",
    "        for texts, y, mask in tqdm(dl2, desc=f\"self-train ep{ep+1}/{SELF_TRAIN_EPOCHS}\"):\n",
    "            enc = tok(list(texts), truncation=True, padding=True, max_length=DOC_MAX_TOK, return_tensors=\"pt\").to(device)\n",
    "            y = y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            logits = model(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "            loss = masked_bce_loss(logits, y, mask)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            total += float(loss.detach().cpu())\n",
    "        avg = total / max(1, len(dl2))\n",
    "        print(f\"[STEP4] epoch {ep+1}: loss={avg:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Final prediction on test\n",
    "    test_probs = predict_probs(model, tok, test_texts, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    test_labels = []\n",
    "    for p in test_probs:\n",
    "        top = np.argsort(-p)[:MAX_LABELS].tolist()\n",
    "        labs = ensure_k_labels(top, parents_of)\n",
    "        test_labels.append(labs)\n",
    "\n",
    "    with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"id\", \"labels\"])\n",
    "        for pid, labs in zip(test_pids, test_labels):\n",
    "            w.writerow([pid, \",\".join(map(str, labs))])\n",
    "\n",
    "    print(f\"[DONE] saved: {SUBMISSION_PATH} | test={len(test_pids)} | labels={MIN_LABELS}-{MAX_LABELS}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
