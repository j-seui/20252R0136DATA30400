{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "Vertex AI Express mode (API Key) + RoBERTa embedding 기반 hierarchical multi-label 분류\n",
    "- Endpoint: https://aiplatform.googleapis.com/v1/publishers/google/models/{model}:generateContent?key=API_KEY\n",
    "  (Express mode는 project/location 없이 global endpoint 사용) :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "조건:\n",
    "  1) gemini-2.0-flash-lite-001 + class/class_related_keywords로 class description 생성 (약 54회 호출)\n",
    "  2) FacebookAI/roberta-base 임베딩: class description & (train+test) 리뷰 텍스트\n",
    "  3) leaf node only cosine similarity로 core class 결정. threshold 이하(애매)면 LLM batch(10개)로 판정\n",
    "  4) 최대 3개의 hierarchical class: parent들과 cosine sim 비교해 greedy로 채택, sim 급감하면 중단\n",
    "  5) test_corpus 결과를 submission.csv [\"id\",\"labels\"]로 저장\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# =========================\n",
    "# User Config\n",
    "# =========================\n",
    "API_KEY = \"AQ.Ab8RN6JUb82VrSIvjvHxX1wie7AH4c-SzUSd6pGwiHcCbRUt3g\"  # <-- Express mode API Key 입력\n",
    "GEMINI_MODEL = \"gemini-2.0-flash-lite-001\"  # 요구사항 준수\n",
    "\n",
    "ROOT_DIR = \"Amazon_products\"\n",
    "CLASSES_PATH = os.path.join(ROOT_DIR, \"classes.txt\")\n",
    "HIER_PATH = os.path.join(ROOT_DIR, \"class_hierarchy.txt\")\n",
    "KW_PATH = os.path.join(ROOT_DIR, \"class_related_keywords.txt\")\n",
    "TRAIN_CORPUS_PATH = os.path.join(ROOT_DIR, \"train\", \"train_corpus.txt\")\n",
    "TEST_CORPUS_PATH = os.path.join(ROOT_DIR, \"test\", \"test_corpus.txt\")\n",
    "\n",
    "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"_artifacts\")\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "CLASS_DESC_JSON = os.path.join(ARTIFACT_DIR, \"class_descriptions.json\")\n",
    "CLASS_EMB_NPY = os.path.join(ARTIFACT_DIR, \"class_embeddings.npy\")\n",
    "REVIEW_EMB_NPY = os.path.join(ARTIFACT_DIR, \"review_embeddings.npy\")\n",
    "REVIEW_META_JSONL = os.path.join(ARTIFACT_DIR, \"review_meta.jsonl\")\n",
    "\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "# API usage guard\n",
    "API_CALL_LIMIT = 1000\n",
    "API_CALL_COUNT = 0\n",
    "\n",
    "# 531 classes -> 약 54회 호출 목표\n",
    "DESC_CHUNK_SIZE = 10\n",
    "\n",
    "# Embedding model\n",
    "EMB_MODEL_NAME = \"FacebookAI/roberta-base\"\n",
    "MAX_SEQ_LEN = 256\n",
    "EMB_BATCH_SIZE = 64\n",
    "\n",
    "# LLM fallback batching\n",
    "TOPK_CANDIDATES_FOR_LLM = 5\n",
    "LLM_BATCH_SIZE = 10\n",
    "\n",
    "# Threshold calibration (train+test 모두 사용)\n",
    "PERCENTILE_FOR_THRESHOLD = 15\n",
    "THRESHOLD_FLOOR = 0.20\n",
    "\n",
    "# Parent greedy selection\n",
    "PARENT_MIN_SIM = 0.15\n",
    "DROP_ABS_DELTA = 0.10      # 절대 감소\n",
    "DROP_REL_RATIO = 0.25      # 상대 감소(25% 이상 감소면 급감으로 판단)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Reproducibility\n",
    "# =========================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def is_int_str(s: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"\\d+\", s.strip()))\n",
    "\n",
    "\n",
    "def safe_json_load(path: str, default):\n",
    "    if not os.path.exists(path):\n",
    "        return default\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def safe_json_save(obj, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def normalize_rows(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    return x / np.clip(n, eps, None)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Load taxonomy files\n",
    "# =========================\n",
    "def load_classes(path: str):\n",
    "    \"\"\"\n",
    "    지원 포맷:\n",
    "      - id<TAB>name\n",
    "      - id name...\n",
    "      - name only (라인 인덱스를 id로 사용)\n",
    "    \"\"\"\n",
    "    id2name = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = re.split(r\"\\t+\", line, maxsplit=1)\n",
    "            if len(parts) == 2 and is_int_str(parts[0]):\n",
    "                cid = int(parts[0])\n",
    "                name = parts[1].strip()\n",
    "            else:\n",
    "                sp = line.split(\" \", 1)\n",
    "                if len(sp) == 2 and is_int_str(sp[0]):\n",
    "                    cid = int(sp[0])\n",
    "                    name = sp[1].strip()\n",
    "                else:\n",
    "                    cid = idx\n",
    "                    name = line.strip()\n",
    "\n",
    "            id2name[cid] = name\n",
    "    return id2name\n",
    "\n",
    "\n",
    "def load_keywords(path: str):\n",
    "    \"\"\"\n",
    "    지원 포맷(유연):\n",
    "      - id<TAB>kw1,kw2,...\n",
    "      - id: kw1, kw2 ...\n",
    "      - id<TAB>kw1 kw2 ...\n",
    "    \"\"\"\n",
    "    id2kws = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            line = line.replace(\":\", \"\\t\")\n",
    "            parts = re.split(r\"\\t+\", line, maxsplit=1)\n",
    "            if len(parts) != 2 or not is_int_str(parts[0]):\n",
    "                continue\n",
    "            cid = int(parts[0])\n",
    "            raw = parts[1].strip()\n",
    "            if \",\" in raw:\n",
    "                kws = [k.strip() for k in raw.split(\",\") if k.strip()]\n",
    "            else:\n",
    "                kws = [k.strip() for k in raw.split() if k.strip()]\n",
    "            id2kws[cid] = kws\n",
    "    return id2kws\n",
    "\n",
    "\n",
    "def load_hierarchy(path: str):\n",
    "    \"\"\"\n",
    "    class_hierarchy.txt에서 (a,b) 페어들을 읽고,\n",
    "    parent->child 또는 child->parent 중 더 그럴듯한 방향(루트가 적고 cycle 적은 쪽)을 선택.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "    pairs = []\n",
    "    for line in lines:\n",
    "        parts = re.split(r\"[\\t, ]+\", line)\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        a, b = parts[0], parts[1]\n",
    "        if is_int_str(a) and is_int_str(b):\n",
    "            pairs.append((int(a), int(b)))\n",
    "\n",
    "    def build(p2c: bool):\n",
    "        parent_of = {}\n",
    "        children_of = {}\n",
    "        for x, y in pairs:\n",
    "            p, c = (x, y) if p2c else (y, x)\n",
    "            children_of.setdefault(p, set()).add(c)\n",
    "            parent_of.setdefault(c, p)  # 단일 parent 가정\n",
    "        return parent_of, children_of\n",
    "\n",
    "    def has_cycle(parent_of):\n",
    "        seen, visiting = set(), set()\n",
    "\n",
    "        def dfs(n):\n",
    "            if n in visiting:\n",
    "                return True\n",
    "            if n in seen:\n",
    "                return False\n",
    "            visiting.add(n)\n",
    "            p = parent_of.get(n)\n",
    "            if p is not None and dfs(p):\n",
    "                return True\n",
    "            visiting.remove(n)\n",
    "            seen.add(n)\n",
    "            return False\n",
    "\n",
    "        for n in list(parent_of.keys()):\n",
    "            if dfs(n):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def count_roots(parent_of, children_of):\n",
    "        nodes = set(parent_of.keys()) | set(children_of.keys())\n",
    "        return sum(1 for n in nodes if n not in parent_of)\n",
    "\n",
    "    p1, c1 = build(True)\n",
    "    p2, c2 = build(False)\n",
    "    cyc1, cyc2 = has_cycle(p1), has_cycle(p2)\n",
    "    r1, r2 = count_roots(p1, c1), count_roots(p2, c2)\n",
    "\n",
    "    if (not cyc1) and cyc2:\n",
    "        return p1, c1\n",
    "    if (not cyc2) and cyc1:\n",
    "        return p2, c2\n",
    "    if cyc1 and cyc2:\n",
    "        return p1, c1\n",
    "    return (p1, c1) if r1 <= r2 else (p2, c2)\n",
    "\n",
    "\n",
    "def get_leaf_nodes(all_class_ids, children_of):\n",
    "    return sorted([cid for cid in all_class_ids if cid not in children_of or len(children_of[cid]) == 0])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Vertex AI Express mode (API key) non-stream generateContent\n",
    "# =========================\n",
    "def vertex_generate_content(prompt: str, temperature: float = 0.2, max_output_tokens: int = 2048) -> str:\n",
    "    \"\"\"\n",
    "    Express mode non-stream endpoint:\n",
    "      POST https://aiplatform.googleapis.com/v1/{model}:generateContent\n",
    "    where {model} is 'publishers/google/models/*'\n",
    "    and API key is passed via ?key=... :contentReference[oaicite:3]{index=3}\n",
    "    \"\"\"\n",
    "    global API_CALL_COUNT\n",
    "    if API_CALL_COUNT >= API_CALL_LIMIT:\n",
    "        raise RuntimeError(f\"API_CALL_LIMIT exceeded ({API_CALL_LIMIT}).\")\n",
    "\n",
    "    url = (\n",
    "        \"https://aiplatform.googleapis.com/v1/publishers/google/models/\"\n",
    "        f\"{GEMINI_MODEL}:generateContent?key={API_KEY}\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxOutputTokens\": max_output_tokens,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    backoff = 2.0\n",
    "    for attempt in range(6):\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=90)\n",
    "        if r.status_code == 200:\n",
    "            API_CALL_COUNT += 1\n",
    "            data = r.json()\n",
    "            return _extract_text_from_generatecontent(data)\n",
    "\n",
    "        if r.status_code in (429, 500, 503):\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 1.7\n",
    "            continue\n",
    "\n",
    "        raise RuntimeError(f\"Vertex generateContent error: {r.status_code} {r.text}\")\n",
    "\n",
    "    raise RuntimeError(\"Vertex generateContent failed after retries.\")\n",
    "\n",
    "\n",
    "def _extract_text_from_generatecontent(data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Non-stream response에서 candidates[0].content.parts[].text를 합침\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cands = data.get(\"candidates\", [])\n",
    "        if not cands:\n",
    "            return json.dumps(data, ensure_ascii=False)\n",
    "        parts = cands[0].get(\"content\", {}).get(\"parts\", [])\n",
    "        out = []\n",
    "        for p in parts:\n",
    "            t = p.get(\"text\")\n",
    "            if t:\n",
    "                out.append(t)\n",
    "        return \"\".join(out).strip()\n",
    "    except Exception:\n",
    "        return json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Class description generation (cached)\n",
    "# =========================\n",
    "def build_desc_prompt(chunk_items):\n",
    "    example = {\"class_id\": 0, \"description\": \"1-3 sentence English description of the category.\"}\n",
    "    lines = []\n",
    "    lines.append(\"You generate short English descriptions for product taxonomy classes.\")\n",
    "    lines.append(\"Return STRICT JSON only. No markdown. No commentary.\")\n",
    "    lines.append(\"Output must be a JSON array; each item: {class_id:int, description:string}.\")\n",
    "    lines.append(f\"Example: {json.dumps(example)}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Classes:\")\n",
    "    for it in chunk_items:\n",
    "        lines.append(json.dumps({\n",
    "            \"class_id\": it[\"class_id\"],\n",
    "            \"class_name\": it[\"class_name\"],\n",
    "            \"related_keywords\": it.get(\"keywords\", [])[:25],\n",
    "            \"parent_name\": it.get(\"parent_name\", None),\n",
    "        }, ensure_ascii=False))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def parse_desc_response(text: str):\n",
    "    m = re.search(r\"\\[[\\s\\S]*\\]\", text)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Description response is not JSON array. Raw:\\n{text[:400]}\")\n",
    "    arr = json.loads(m.group(0))\n",
    "    out = {}\n",
    "    for obj in arr:\n",
    "        out[int(obj[\"class_id\"])] = str(obj[\"description\"]).strip()\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_class_descriptions(id2name, id2kws, parent_of):\n",
    "    existing = safe_json_load(CLASS_DESC_JSON, default={})\n",
    "    existing_norm = {}\n",
    "    for k, v in existing.items():\n",
    "        try:\n",
    "            existing_norm[int(k)] = v\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    all_ids = sorted(id2name.keys())\n",
    "    missing = [cid for cid in all_ids if cid not in existing_norm]\n",
    "\n",
    "    if not missing:\n",
    "        return existing_norm\n",
    "\n",
    "    def parent_name(cid):\n",
    "        p = parent_of.get(cid)\n",
    "        return id2name.get(p) if p is not None else None\n",
    "\n",
    "    chunks = [missing[i:i + DESC_CHUNK_SIZE] for i in range(0, len(missing), DESC_CHUNK_SIZE)]\n",
    "    for chunk in tqdm(chunks, desc=\"Generating class descriptions (generateContent)\"):\n",
    "        chunk_items = []\n",
    "        for cid in chunk:\n",
    "            chunk_items.append({\n",
    "                \"class_id\": cid,\n",
    "                \"class_name\": id2name.get(cid, f\"class_{cid}\"),\n",
    "                \"keywords\": id2kws.get(cid, []),\n",
    "                \"parent_name\": parent_name(cid),\n",
    "            })\n",
    "\n",
    "        prompt = build_desc_prompt(chunk_items)\n",
    "        resp = vertex_generate_content(prompt, temperature=0.2, max_output_tokens=2048)\n",
    "        parsed = parse_desc_response(resp)\n",
    "\n",
    "        for cid, desc in parsed.items():\n",
    "            existing_norm[cid] = {\n",
    "                \"name\": id2name.get(cid, f\"class_{cid}\"),\n",
    "                \"keywords\": id2kws.get(cid, []),\n",
    "                \"description\": desc,\n",
    "            }\n",
    "\n",
    "        safe_json_save({str(k): v for k, v in existing_norm.items()}, CLASS_DESC_JSON)\n",
    "\n",
    "    return existing_norm\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) RoBERTa embeddings\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "    summed = torch.sum(last_hidden_state * mask, dim=1)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "\n",
    "def embed_texts_roberta(texts, tokenizer, model, device, batch_size=64, max_len=256):\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        out = model(**enc)\n",
    "        pooled = mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "        pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "        embs.append(pooled.cpu().numpy())\n",
    "    return np.vstack(embs)\n",
    "\n",
    "\n",
    "def load_corpus_any(path: str, split_name: str):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "            elif len(parts) >= 3:\n",
    "                pid = parts[0]\n",
    "                text = parts[-1]\n",
    "            else:\n",
    "                continue\n",
    "            rows.append({\"split\": split_name, \"pid\": pid, \"text\": text})\n",
    "    return rows\n",
    "\n",
    "\n",
    "def build_or_load_review_store():\n",
    "    if os.path.exists(REVIEW_META_JSONL):\n",
    "        train_rows, test_rows, all_rows = [], [], []\n",
    "        with open(REVIEW_META_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                r = json.loads(line)\n",
    "                all_rows.append(r)\n",
    "                (train_rows if r.get(\"split\") == \"train\" else test_rows).append(r)\n",
    "        return train_rows, test_rows, all_rows\n",
    "\n",
    "    train_rows = load_corpus_any(TRAIN_CORPUS_PATH, \"train\")\n",
    "    test_rows = load_corpus_any(TEST_CORPUS_PATH, \"test\")\n",
    "    all_rows = train_rows + test_rows\n",
    "\n",
    "    with open(REVIEW_META_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in all_rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    return train_rows, test_rows, all_rows\n",
    "\n",
    "\n",
    "def build_or_load_embeddings(class_desc_map, all_reviews):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMB_MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(EMB_MODEL_NAME).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if os.path.exists(CLASS_EMB_NPY):\n",
    "        class_emb = np.load(CLASS_EMB_NPY)\n",
    "    else:\n",
    "        max_id = max(class_desc_map.keys())\n",
    "        class_texts = []\n",
    "        for cid in range(max_id + 1):\n",
    "            info = class_desc_map.get(cid)\n",
    "            if info is None:\n",
    "                class_texts.append(f\"class_{cid}\")\n",
    "            else:\n",
    "                nm = info.get(\"name\", f\"class_{cid}\")\n",
    "                kws = \", \".join(info.get(\"keywords\", [])[:20])\n",
    "                desc = info.get(\"description\", \"\")\n",
    "                class_texts.append(f\"Class: {nm}. Keywords: {kws}. Description: {desc}\")\n",
    "\n",
    "        class_emb = embed_texts_roberta(\n",
    "            class_texts, tokenizer, model, device,\n",
    "            batch_size=EMB_BATCH_SIZE, max_len=MAX_SEQ_LEN\n",
    "        )\n",
    "        class_emb = normalize_rows(class_emb)\n",
    "        np.save(CLASS_EMB_NPY, class_emb)\n",
    "\n",
    "    if os.path.exists(REVIEW_EMB_NPY):\n",
    "        review_emb = np.load(REVIEW_EMB_NPY)\n",
    "    else:\n",
    "        texts = [r[\"text\"] for r in all_reviews]\n",
    "        review_emb = embed_texts_roberta(\n",
    "            texts, tokenizer, model, device,\n",
    "            batch_size=EMB_BATCH_SIZE, max_len=MAX_SEQ_LEN\n",
    "        )\n",
    "        review_emb = normalize_rows(review_emb)\n",
    "        np.save(REVIEW_EMB_NPY, review_emb)\n",
    "\n",
    "    return normalize_rows(class_emb), normalize_rows(review_emb)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Core leaf selection + LLM fallback\n",
    "# =========================\n",
    "def calibrate_threshold(leaf_emb: np.ndarray, review_emb_all: np.ndarray) -> float:\n",
    "    max_sims = []\n",
    "    leaf_t = leaf_emb.T\n",
    "    bs = 2048\n",
    "    for i in range(0, review_emb_all.shape[0], bs):\n",
    "        chunk = review_emb_all[i:i+bs]\n",
    "        sims = chunk @ leaf_t\n",
    "        max_sims.append(sims.max(axis=1))\n",
    "    max_sims = np.concatenate(max_sims, axis=0)\n",
    "    thr = float(np.percentile(max_sims, PERCENTILE_FOR_THRESHOLD))\n",
    "    return max(thr, THRESHOLD_FLOOR)\n",
    "\n",
    "\n",
    "def build_llm_batch_prompt(batch_items, id2name, class_desc_map):\n",
    "    lines = []\n",
    "    lines.append(\"You classify each review into ONE best leaf class among provided candidates.\")\n",
    "    lines.append(\"Return STRICT JSON only. Output must be a JSON array of objects: {idx:int, chosen_class_id:int}.\")\n",
    "    lines.append(\"No markdown. No explanations.\")\n",
    "    lines.append(\"\")\n",
    "    for item in batch_items:\n",
    "        cands = []\n",
    "        for cid, score in item[\"candidates\"]:\n",
    "            info = class_desc_map.get(cid, {})\n",
    "            cands.append({\n",
    "                \"class_id\": cid,\n",
    "                \"class_name\": id2name.get(cid, info.get(\"name\", f\"class_{cid}\")),\n",
    "                \"description\": info.get(\"description\", \"\"),\n",
    "                \"similarity_hint\": round(float(score), 4),\n",
    "            })\n",
    "        lines.append(json.dumps({\n",
    "            \"idx\": item[\"idx\"],\n",
    "            \"review_text\": item[\"text\"],\n",
    "            \"candidates\": cands,\n",
    "        }, ensure_ascii=False))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def parse_llm_choice_response(text: str):\n",
    "    m = re.search(r\"\\[[\\s\\S]*\\]\", text)\n",
    "    if not m:\n",
    "        raise ValueError(f\"LLM choice response is not JSON array. Raw:\\n{text[:400]}\")\n",
    "    arr = json.loads(m.group(0))\n",
    "    out = {}\n",
    "    for obj in arr:\n",
    "        out[int(obj[\"idx\"])] = int(obj[\"chosen_class_id\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "def infer_core_leaf_classes(leaf_ids, leaf_emb, review_emb_all, all_reviews, id2name, class_desc_map):\n",
    "    global API_CALL_COUNT\n",
    "\n",
    "    leaf_t = leaf_emb.T\n",
    "    n = review_emb_all.shape[0]\n",
    "    best_leaf = np.empty(n, dtype=np.int32)\n",
    "    best_sim = np.empty(n, dtype=np.float32)\n",
    "\n",
    "    bs = 2048\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Core leaf by cosine (top1)\"):\n",
    "        chunk = review_emb_all[i:i+bs]\n",
    "        sims = chunk @ leaf_t\n",
    "        arg = sims.argmax(axis=1)\n",
    "        best_leaf[i:i+bs] = np.array([leaf_ids[int(p)] for p in arg], dtype=np.int32)\n",
    "        best_sim[i:i+bs] = sims.max(axis=1).astype(np.float32)\n",
    "\n",
    "    thr = calibrate_threshold(leaf_emb, review_emb_all)\n",
    "    print(f\"[Threshold] ambiguity threshold={thr:.4f} (percentile={PERCENTILE_FOR_THRESHOLD}, floor={THRESHOLD_FLOOR})\")\n",
    "\n",
    "    ambiguous_idx = np.where(best_sim < thr)[0].tolist()\n",
    "    print(f\"[Ambiguous] {len(ambiguous_idx)} / {n} below threshold -> LLM fallback (batched {LLM_BATCH_SIZE}).\")\n",
    "\n",
    "    core_leaf_for_idx = {i: int(best_leaf[i]) for i in range(n)}\n",
    "\n",
    "    if not ambiguous_idx:\n",
    "        return core_leaf_for_idx\n",
    "\n",
    "    remaining = API_CALL_LIMIT - API_CALL_COUNT\n",
    "    needed_calls = math.ceil(len(ambiguous_idx) / LLM_BATCH_SIZE)\n",
    "    if needed_calls > remaining:\n",
    "        print(f\"[Warn] LLM fallback needs {needed_calls} calls but only {remaining} remain. \"\n",
    "              f\"Remaining ambiguous will keep cosine-top1 result.\")\n",
    "        ambiguous_idx = ambiguous_idx[:remaining * LLM_BATCH_SIZE]\n",
    "\n",
    "    batches = [ambiguous_idx[i:i+LLM_BATCH_SIZE] for i in range(0, len(ambiguous_idx), LLM_BATCH_SIZE)]\n",
    "    for b in tqdm(batches, desc=\"LLM fallback (generateContent)\"):\n",
    "        batch_items = []\n",
    "        for idx in b:\n",
    "            rv = review_emb_all[idx]\n",
    "            sims = rv @ leaf_t\n",
    "            topk_pos = np.argpartition(-sims, TOPK_CANDIDATES_FOR_LLM - 1)[:TOPK_CANDIDATES_FOR_LLM]\n",
    "            topk_pos = topk_pos[np.argsort(-sims[topk_pos])]\n",
    "            cands = [(leaf_ids[int(p)], float(sims[int(p)])) for p in topk_pos]\n",
    "            batch_items.append({\"idx\": idx, \"text\": all_reviews[idx][\"text\"], \"candidates\": cands})\n",
    "\n",
    "        prompt = build_llm_batch_prompt(batch_items, id2name, class_desc_map)\n",
    "        resp = vertex_generate_content(prompt, temperature=0.0, max_output_tokens=2048)\n",
    "        chosen_map = parse_llm_choice_response(resp)\n",
    "\n",
    "        for i2, cid in chosen_map.items():\n",
    "            core_leaf_for_idx[int(i2)] = int(cid)\n",
    "\n",
    "    return core_leaf_for_idx\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Greedy parent selection (max 3 labels)\n",
    "# =========================\n",
    "def select_hierarchical_labels(review_vec_norm, core_leaf_cid, parent_of, class_emb_all):\n",
    "    labels = [int(core_leaf_cid)]\n",
    "    child = int(core_leaf_cid)\n",
    "\n",
    "    prev_sim = float(review_vec_norm @ class_emb_all[child])\n",
    "\n",
    "    while len(labels) < 3:\n",
    "        p = parent_of.get(child)\n",
    "        if p is None:\n",
    "            break\n",
    "        p = int(p)\n",
    "\n",
    "        p_sim = float(review_vec_norm @ class_emb_all[p])\n",
    "\n",
    "        if p_sim < PARENT_MIN_SIM:\n",
    "            break\n",
    "\n",
    "        abs_drop = (prev_sim - p_sim)\n",
    "        rel_drop = abs_drop / max(prev_sim, 1e-6)\n",
    "        if abs_drop >= DROP_ABS_DELTA or rel_drop >= DROP_REL_RATIO:\n",
    "            break\n",
    "\n",
    "        labels.append(p)\n",
    "        child = p\n",
    "        prev_sim = p_sim\n",
    "\n",
    "    labels = sorted(set(labels))\n",
    "    return labels[:3]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Submission writer\n",
    "# =========================\n",
    "def write_submission(test_rows, all_rows, review_emb_all, leaf_ids, leaf_emb, pid_to_labels, out_path):\n",
    "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"id\", \"labels\"])\n",
    "        for r in test_rows:\n",
    "            pid = r[\"pid\"]\n",
    "            labels = pid_to_labels.get(pid, [])\n",
    "            if not labels:\n",
    "                # fallback: 최소 1개 라벨\n",
    "                idx = None\n",
    "                for i, rr in enumerate(all_rows):\n",
    "                    if rr.get(\"split\") == \"test\" and rr.get(\"pid\") == pid:\n",
    "                        idx = i\n",
    "                        break\n",
    "                if idx is not None:\n",
    "                    sims = review_emb_all[idx] @ leaf_emb.T\n",
    "                    cid = leaf_ids[int(np.argmax(sims))]\n",
    "                    labels = [int(cid)]\n",
    "            w.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    global API_CALL_COUNT\n",
    "\n",
    "    id2name = load_classes(CLASSES_PATH)\n",
    "    id2kws = load_keywords(KW_PATH)\n",
    "    parent_of, children_of = load_hierarchy(HIER_PATH)\n",
    "\n",
    "    all_class_ids = sorted(id2name.keys())\n",
    "    leaf_ids = get_leaf_nodes(all_class_ids, children_of)\n",
    "    print(f\"[Taxonomy] classes={len(all_class_ids)}, leaf={len(leaf_ids)}\")\n",
    "\n",
    "    # 1) class descriptions (cached)\n",
    "    class_desc_map = generate_class_descriptions(id2name, id2kws, parent_of)\n",
    "    print(f\"[Descriptions] ready={len(class_desc_map)} | API used={API_CALL_COUNT}/{API_CALL_LIMIT}\")\n",
    "\n",
    "    # train+test 모두 사용\n",
    "    train_rows, test_rows, all_rows = build_or_load_review_store()\n",
    "    print(f\"[Corpus] train={len(train_rows)}, test={len(test_rows)}, all={len(all_rows)}\")\n",
    "\n",
    "    # 2) embeddings (cached)\n",
    "    class_emb_all, review_emb_all = build_or_load_embeddings(class_desc_map, all_rows)\n",
    "    class_emb_all = normalize_rows(class_emb_all)\n",
    "    review_emb_all = normalize_rows(review_emb_all)\n",
    "\n",
    "    leaf_emb = class_emb_all[leaf_ids]\n",
    "\n",
    "    # 3) core leaf inference (+ LLM fallback)\n",
    "    core_leaf_for_idx = infer_core_leaf_classes(\n",
    "        leaf_ids, leaf_emb, review_emb_all, all_rows, id2name, class_desc_map\n",
    "    )\n",
    "    print(f\"[Core leaf] done | API used={API_CALL_COUNT}/{API_CALL_LIMIT}\")\n",
    "\n",
    "    # 4) hierarchical labels for test only\n",
    "    pid_to_labels = {}\n",
    "    for i, r in tqdm(list(enumerate(all_rows)), desc=\"Hierarchical labeling\"):\n",
    "        if r.get(\"split\") != \"test\":\n",
    "            continue\n",
    "        pid = r[\"pid\"]\n",
    "        rv = review_emb_all[i]\n",
    "        core = core_leaf_for_idx[i]\n",
    "        labels = select_hierarchical_labels(rv, core, parent_of, class_emb_all)\n",
    "        pid_to_labels[pid] = labels\n",
    "\n",
    "    # 5) save submission\n",
    "    write_submission(test_rows, all_rows, review_emb_all, leaf_ids, leaf_emb, pid_to_labels, SUBMISSION_PATH)\n",
    "    print(f\"[Done] submission saved: {SUBMISSION_PATH}\")\n",
    "    print(f\"[API usage] total calls: {API_CALL_COUNT}/{API_CALL_LIMIT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
