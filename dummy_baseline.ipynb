{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19658/19658 [00:00<00:00, 190266.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# TaxoClass-inspired baseline (adapted to your file structure)\n",
    "# - No labeled data used\n",
    "# - Uses: class names + taxonomy + class-related keywords + unlabeled corpus\n",
    "#\n",
    "# Pipeline:\n",
    "#   1) Build doc-class similarity (TF-IDF cosine) using class text = name + keywords\n",
    "#   2) Core class mining:\n",
    "#        - top-down candidate expansion on taxonomy\n",
    "#        - confidence = sim(c) - max(sim(parent), max sim(siblings))\n",
    "#        - keep cores if conf>0 and conf >= median_conf_for_class\n",
    "#   3) Silver labels: core + ancestors => enforce 2~3 labels\n",
    "#   4) Train multi-label classifier (One-vs-Rest Logistic Regression)\n",
    "#   5) Self-training (optional): refine pseudo labels using model probabilities\n",
    "#   6) Predict test, write submission.csv\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# ------------------------\n",
    "# Reproducibility\n",
    "# ------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ------------------------\n",
    "# Paths (fixed by your file structure)\n",
    "# ------------------------\n",
    "BASE_DIR = \"Amazon_products\"\n",
    "CLASSES_PATH = os.path.join(BASE_DIR, \"classes.txt\")\n",
    "HIERARCHY_PATH = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "KEYWORDS_PATH = os.path.join(BASE_DIR, \"class_related_keywords.txt\")\n",
    "\n",
    "TRAIN_CORPUS_PATH = os.path.join(BASE_DIR, \"train\", \"train_corpus.txt\")  # pid \\t text\n",
    "TEST_CORPUS_PATH = os.path.join(BASE_DIR, \"test\", \"test_corpus.txt\")     # pid \\t text\n",
    "\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "# ------------------------\n",
    "# Constants\n",
    "# ------------------------\n",
    "NUM_CLASSES = 531  # 0..530\n",
    "MIN_LABELS = 2\n",
    "MAX_LABELS = 3\n",
    "\n",
    "# ------------------------\n",
    "# I/O helpers\n",
    "# ------------------------\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load corpus into (pid_list, text_list). Format: pid\\\\ttext\"\"\"\n",
    "    pids, texts = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pids.append(pid)\n",
    "                texts.append(text)\n",
    "    return pids, texts\n",
    "\n",
    "def load_classes(path, expected_num=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Robust loader for classes.txt.\n",
    "    Supports either:\n",
    "      - one class name per line (id = line index)\n",
    "      - \"id\\\\tname\" or \"id,name\"\n",
    "    Returns: id2name dict\n",
    "    \"\"\"\n",
    "    id2name = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            parts = re.split(r\"[\\t,]\", s, maxsplit=1)\n",
    "            if len(parts) == 2 and parts[0].strip().isdigit():\n",
    "                cid = int(parts[0].strip())\n",
    "                name = parts[1].strip()\n",
    "                id2name[cid] = name\n",
    "            else:\n",
    "                id2name[i] = s\n",
    "\n",
    "    # pad missing ids defensively\n",
    "    for cid in range(expected_num):\n",
    "        if cid not in id2name:\n",
    "            id2name[cid] = f\"class_{cid}\"\n",
    "    return id2name\n",
    "\n",
    "def load_keywords(path, id2name):\n",
    "    \"\"\"\n",
    "    Robust loader for class_related_keywords.txt.\n",
    "    Common formats handled:\n",
    "      - \"cid\\\\tkw1,kw2,kw3\"\n",
    "      - \"cid\\\\tkw1\\\\tkw2\\\\tkw3\"\n",
    "      - \"cid,kw1,kw2,...\"\n",
    "    Returns: kw[cid] = [kw...]\n",
    "    \"\"\"\n",
    "    kw = {cid: [] for cid in id2name.keys()}\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "\n",
    "            # First split by tab; if only 1 field, split by comma.\n",
    "            tab_parts = s.split(\"\\t\")\n",
    "            if len(tab_parts) == 1:\n",
    "                parts = s.split(\",\")\n",
    "            else:\n",
    "                parts = tab_parts\n",
    "\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "\n",
    "            cid_raw = parts[0].strip()\n",
    "            if not cid_raw.isdigit():\n",
    "                continue\n",
    "            cid = int(cid_raw)\n",
    "            if cid not in kw:\n",
    "                continue\n",
    "\n",
    "            rest = [p.strip() for p in parts[1:] if p.strip()]\n",
    "            # If rest is a single \"kw1,kw2,kw3\" string, split again by comma/;/\n",
    "            expanded = []\n",
    "            for r in rest:\n",
    "                expanded.extend([x.strip() for x in re.split(r\"[,;/|]\", r) if x.strip()])\n",
    "            # de-dup while preserving order\n",
    "            seen = set()\n",
    "            deduped = []\n",
    "            for x in expanded:\n",
    "                if x not in seen:\n",
    "                    deduped.append(x)\n",
    "                    seen.add(x)\n",
    "\n",
    "            kw[cid] = deduped\n",
    "\n",
    "    return kw\n",
    "\n",
    "def load_hierarchy(path, id2name):\n",
    "    \"\"\"\n",
    "    Load taxonomy edges from class_hierarchy.txt.\n",
    "    Expected per line: parent_id<sep>child_id  (sep = tab or comma)\n",
    "    (If the file is name-based, this also tries to map by class names.)\n",
    "    Returns:\n",
    "      roots: [cid...]\n",
    "      parents_of: dict child -> sorted list of parents (can be multiple)\n",
    "      children_of: dict parent -> sorted list of children\n",
    "    \"\"\"\n",
    "    name2id = {v: k for k, v in id2name.items()}\n",
    "    parents_of = {cid: set() for cid in id2name.keys()}\n",
    "    children_of = {cid: set() for cid in id2name.keys()}\n",
    "\n",
    "    def to_id(x):\n",
    "        x = x.strip()\n",
    "        if x.isdigit():\n",
    "            return int(x)\n",
    "        return name2id.get(x, None)\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            parts = re.split(r\"[\\t,]\", s)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            p = to_id(parts[0])\n",
    "            c = to_id(parts[1])\n",
    "            if p is None or c is None or p == c:\n",
    "                continue\n",
    "            parents_of[c].add(p)\n",
    "            children_of[p].add(c)\n",
    "\n",
    "    roots = [cid for cid in id2name.keys() if len(parents_of[cid]) == 0]\n",
    "    parents_of = {k: sorted(list(v)) for k, v in parents_of.items()}\n",
    "    children_of = {k: sorted(list(v)) for k, v in children_of.items()}\n",
    "    return roots, parents_of, children_of\n",
    "\n",
    "# ------------------------\n",
    "# Taxonomy utilities\n",
    "# ------------------------\n",
    "def get_ancestors(cid, parents_of, max_steps=10):\n",
    "    \"\"\"Return a deterministic ancestor chain using the smallest parent when multiple exist.\"\"\"\n",
    "    anc = []\n",
    "    cur = cid\n",
    "    for _ in range(max_steps):\n",
    "        plist = parents_of.get(cur, [])\n",
    "        if not plist:\n",
    "            break\n",
    "        p = plist[0]\n",
    "        anc.append(p)\n",
    "        cur = p\n",
    "    return anc\n",
    "\n",
    "# ------------------------\n",
    "# Build class texts (name + keywords)\n",
    "# ------------------------\n",
    "def build_class_texts(id2name, kw):\n",
    "    class_texts = []\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        name = id2name.get(cid, f\"class_{cid}\")\n",
    "        kws = kw.get(cid, [])[:30]  # cap to reduce noise\n",
    "        class_texts.append(name + \" \" + \" \".join(kws))\n",
    "    return class_texts\n",
    "\n",
    "# ------------------------\n",
    "# Core mining (TaxoClass-inspired)\n",
    "# ------------------------\n",
    "def mine_core_classes(\n",
    "    X_docs, X_class, roots, parents_of, children_of,\n",
    "    top_k=3, max_nodes_per_doc=60, batch_size=512\n",
    "):\n",
    "    \"\"\"\n",
    "    X_docs: (N,V) sparse\n",
    "    X_class: (C,V) sparse\n",
    "    Uses:\n",
    "      sim(d,c) = dot(tfidf(d), tfidf(class))\n",
    "      candidates: top-down expansion from roots\n",
    "      confidence(d,c) = sim(d,c) - max(sim(parent), max sim(siblings))\n",
    "      keep if conf>0 and conf >= median_conf[c]\n",
    "    Returns:\n",
    "      doc_cores: list[list[cid]]\n",
    "    \"\"\"\n",
    "    n_docs = X_docs.shape[0]\n",
    "    # store conf values per class for median\n",
    "    conf_lists = [[] for _ in range(NUM_CLASSES)]\n",
    "    doc_candidate_confs = [None] * n_docs\n",
    "\n",
    "    X_class_T = X_class.T  # (V,C) for fast dot: X_docs @ X_class_T\n",
    "\n",
    "    # pass 1: collect confidences\n",
    "    for start in tqdm(range(0, n_docs, batch_size), desc=\"Core mining pass1 (collect conf)\"):\n",
    "        end = min(start + batch_size, n_docs)\n",
    "        sims = (X_docs[start:end] @ X_class_T).toarray().astype(np.float32)  # (b,C)\n",
    "\n",
    "        for i in range(end - start):\n",
    "            sim = sims[i]\n",
    "            candidates = set()\n",
    "            frontier = list(roots)\n",
    "            visited = set()\n",
    "            steps = 0\n",
    "\n",
    "            while frontier and len(visited) < max_nodes_per_doc:\n",
    "                node = frontier.pop(0)\n",
    "                if node in visited:\n",
    "                    continue\n",
    "                visited.add(node)\n",
    "                candidates.add(node)\n",
    "\n",
    "                ch = children_of.get(node, [])\n",
    "                if ch:\n",
    "                    ch_sorted = sorted(ch, key=lambda c: sim[c], reverse=True)[:top_k]\n",
    "                    for c in ch_sorted:\n",
    "                        if c not in visited:\n",
    "                            frontier.append(c)\n",
    "\n",
    "                steps += 1\n",
    "                if steps >= max_nodes_per_doc:\n",
    "                    break\n",
    "\n",
    "            cand_confs = []\n",
    "            for c in candidates:\n",
    "                plist = parents_of.get(c, [])\n",
    "                p = plist[0] if plist else None\n",
    "                parent_sim = sim[p] if p is not None else -1e9\n",
    "\n",
    "                sib_max = -1e9\n",
    "                if p is not None:\n",
    "                    for s in children_of.get(p, []):\n",
    "                        if s == c:\n",
    "                            continue\n",
    "                        if sim[s] > sib_max:\n",
    "                            sib_max = sim[s]\n",
    "\n",
    "                baseline = max(parent_sim, sib_max)\n",
    "                conf = float(sim[c] - baseline)\n",
    "\n",
    "                cand_confs.append((c, conf))\n",
    "                if conf > 0:\n",
    "                    conf_lists[c].append(conf)\n",
    "\n",
    "            doc_candidate_confs[start + i] = cand_confs\n",
    "\n",
    "    # per-class median confidence\n",
    "    med = np.full(NUM_CLASSES, -np.inf, dtype=np.float32)\n",
    "    for c in range(NUM_CLASSES):\n",
    "        if conf_lists[c]:\n",
    "            med[c] = float(np.median(np.array(conf_lists[c], dtype=np.float32)))\n",
    "\n",
    "    # pass 2: select top cores per doc\n",
    "    doc_cores = []\n",
    "    for cand_confs in tqdm(doc_candidate_confs, desc=\"Core mining pass2 (select cores)\"):\n",
    "        if not cand_confs:\n",
    "            doc_cores.append([])\n",
    "            continue\n",
    "        filtered = [(c, conf) for (c, conf) in cand_confs if conf > 0 and conf >= med[c]]\n",
    "        filtered.sort(key=lambda x: x[1], reverse=True)\n",
    "        cores = [c for (c, _) in filtered[:2]]  # max 2 cores\n",
    "        doc_cores.append(cores)\n",
    "\n",
    "    return doc_cores\n",
    "\n",
    "# ------------------------\n",
    "# Convert cores -> final 2~3 labels (silver / prediction formatting)\n",
    "# ------------------------\n",
    "def cores_to_labels(cores, parents_of):\n",
    "    labels = []\n",
    "    for c in cores:\n",
    "        labels.append(c)\n",
    "\n",
    "    # add ancestors until reaching MAX_LABELS\n",
    "    for c in cores:\n",
    "        for a in get_ancestors(c, parents_of, max_steps=5):\n",
    "            if len(labels) >= MAX_LABELS:\n",
    "                break\n",
    "            labels.append(a)\n",
    "        if len(labels) >= MAX_LABELS:\n",
    "            break\n",
    "\n",
    "    # dedup preserving order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for x in labels:\n",
    "        if x not in seen:\n",
    "            uniq.append(x)\n",
    "            seen.add(x)\n",
    "    labels = uniq\n",
    "\n",
    "    # ensure at least MIN_LABELS\n",
    "    if len(labels) < MIN_LABELS:\n",
    "        if cores:\n",
    "            for a in get_ancestors(cores[0], parents_of, max_steps=10):\n",
    "                if a not in seen:\n",
    "                    labels.append(a)\n",
    "                    seen.add(a)\n",
    "                if len(labels) >= MIN_LABELS:\n",
    "                    break\n",
    "\n",
    "    # still short: deterministic pad with next ids (rare)\n",
    "    if len(labels) < MIN_LABELS:\n",
    "        for k in range(NUM_CLASSES):\n",
    "            if k not in seen:\n",
    "                labels.append(k)\n",
    "                seen.add(k)\n",
    "            if len(labels) >= MIN_LABELS:\n",
    "                break\n",
    "\n",
    "    # clip and sort for submission\n",
    "    labels = labels[:MAX_LABELS]\n",
    "    return sorted(labels)\n",
    "\n",
    "# ------------------------\n",
    "# Self-training (simple, stable)\n",
    "# ------------------------\n",
    "def self_training(model, X_all_docs, parents_of, rounds=2, min_prob=0.20):\n",
    "    \"\"\"\n",
    "    For each round:\n",
    "      - predict_proba on all docs\n",
    "      - pseudo core = top-1 (plus optional 2nd if close)\n",
    "      - convert to 2~3 labels via ancestors\n",
    "      - re-fit model on pseudo labels\n",
    "    \"\"\"\n",
    "    for _ in range(rounds):\n",
    "        probs = model.predict_proba(X_all_docs)  # (N,C)\n",
    "        pseudo = []\n",
    "        for i in range(probs.shape[0]):\n",
    "            p = probs[i]\n",
    "            top = int(np.argmax(p))\n",
    "            topk = np.argsort(-p)[:5].tolist()\n",
    "\n",
    "            cores = [top]\n",
    "            # optional 2nd core if not too far\n",
    "            for c in topk[1:]:\n",
    "                if p[top] >= min_prob and p[c] >= (p[top] * 0.75):\n",
    "                    cores.append(c)\n",
    "                    break\n",
    "            labs = cores_to_labels(cores, parents_of)\n",
    "            pseudo.append(labs)\n",
    "\n",
    "        mlb = MultiLabelBinarizer(classes=list(range(NUM_CLASSES)))\n",
    "        Y = mlb.fit_transform(pseudo)\n",
    "        model.fit(X_all_docs, Y)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ------------------------\n",
    "# Main\n",
    "# ------------------------\n",
    "def main():\n",
    "    # sanity checks\n",
    "    for p in [CLASSES_PATH, HIERARCHY_PATH, KEYWORDS_PATH, TRAIN_CORPUS_PATH, TEST_CORPUS_PATH]:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "    # load resources\n",
    "    id2name = load_classes(CLASSES_PATH, expected_num=NUM_CLASSES)\n",
    "    kw = load_keywords(KEYWORDS_PATH, id2name)\n",
    "    roots, parents_of, children_of = load_hierarchy(HIERARCHY_PATH, id2name)\n",
    "\n",
    "    # load corpora\n",
    "    train_pids, train_texts = load_corpus(TRAIN_CORPUS_PATH)\n",
    "    test_pids, test_texts = load_corpus(TEST_CORPUS_PATH)\n",
    "\n",
    "    # build class texts\n",
    "    class_texts = build_class_texts(id2name, kw)\n",
    "\n",
    "    # vectorize in a shared TF-IDF space (docs + class texts)\n",
    "    # NOTE: we include test texts to align with the project setting (test corpus allowed for training).\n",
    "    all_texts_for_vocab = train_texts + test_texts + class_texts\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=200000,\n",
    "        min_df=2,\n",
    "        stop_words=\"english\",\n",
    "    )\n",
    "    X_all = vectorizer.fit_transform(all_texts_for_vocab)\n",
    "\n",
    "    n_train = len(train_texts)\n",
    "    n_test = len(test_texts)\n",
    "\n",
    "    X_train = X_all[:n_train]\n",
    "    X_test = X_all[n_train:n_train + n_test]\n",
    "    X_class = X_all[n_train + n_test:]  # (C,V) in same TF-IDF space\n",
    "\n",
    "    # core mining on train corpus to create silver labels\n",
    "    doc_cores = mine_core_classes(\n",
    "        X_docs=X_train,\n",
    "        X_class=X_class,\n",
    "        roots=roots,\n",
    "        parents_of=parents_of,\n",
    "        children_of=children_of,\n",
    "        top_k=3,\n",
    "        max_nodes_per_doc=60,\n",
    "        batch_size=512\n",
    "    )\n",
    "\n",
    "    silver_labels = []\n",
    "    used_idx = []\n",
    "    for i, cores in enumerate(doc_cores):\n",
    "        if not cores:\n",
    "            continue\n",
    "        labs = cores_to_labels(cores, parents_of)\n",
    "        silver_labels.append(labs)\n",
    "        used_idx.append(i)\n",
    "\n",
    "    if len(used_idx) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"No silver-labeled documents found. \"\n",
    "            \"Check that class_hierarchy / keywords parsing is correct.\"\n",
    "        )\n",
    "\n",
    "    X_silver = X_train[used_idx]\n",
    "    mlb = MultiLabelBinarizer(classes=list(range(NUM_CLASSES)))\n",
    "    Y_silver = mlb.fit_transform(silver_labels)\n",
    "\n",
    "    # train multi-label classifier\n",
    "    base = LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        max_iter=250,\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    model = OneVsRestClassifier(base, n_jobs=-1)\n",
    "    model.fit(X_silver, Y_silver)\n",
    "\n",
    "    # self-training on train+test texts (allowed per project statement)\n",
    "    X_all_docs = vectorizer.transform(train_texts + test_texts)\n",
    "    model = self_training(model, X_all_docs, parents_of, rounds=2, min_prob=0.20)\n",
    "\n",
    "    # inference on test\n",
    "    probs_test = model.predict_proba(X_test)  # (Ntest,C)\n",
    "\n",
    "    # format predictions: enforce 2~3 labels\n",
    "    all_labels = []\n",
    "    for i in range(n_test):\n",
    "        p = probs_test[i]\n",
    "        topk = np.argsort(-p)[:5].tolist()\n",
    "        cores = [topk[0]]\n",
    "        for c in topk[1:]:\n",
    "            if p[c] >= (p[topk[0]] * 0.75):\n",
    "                cores.append(c)\n",
    "                break\n",
    "\n",
    "        labels = cores_to_labels(cores, parents_of)\n",
    "\n",
    "        # final enforcement (should already satisfy)\n",
    "        if len(labels) < MIN_LABELS:\n",
    "            for c in topk:\n",
    "                if c not in labels:\n",
    "                    labels.append(c)\n",
    "                if len(labels) >= MIN_LABELS:\n",
    "                    break\n",
    "            labels = sorted(labels)[:MAX_LABELS]\n",
    "        if len(labels) > MAX_LABELS:\n",
    "            labels = labels[:MAX_LABELS]\n",
    "\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # write submission\n",
    "    with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"pid\", \"labels\"])\n",
    "        for pid, labels in zip(test_pids, all_labels):\n",
    "            w.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "    print(f\"Saved: {SUBMISSION_PATH}\")\n",
    "    print(f\"Test samples: {n_test} | labels per sample: {MIN_LABELS}-{MAX_LABELS}\")\n",
    "    print(f\"Silver-labeled train docs used: {len(used_idx)} / {n_train}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
