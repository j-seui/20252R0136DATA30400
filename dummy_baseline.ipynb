{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19658/19658 [00:00<00:00, 190266.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------\n",
    "# CONFIG\n",
    "# ------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_DIR = \"Amazon_products\"\n",
    "CLASSES_PATH   = os.path.join(BASE_DIR, \"classes.txt\")\n",
    "HIER_PATH      = os.path.join(BASE_DIR, \"class_hierarchy.txt\")\n",
    "KEYWORDS_PATH  = os.path.join(BASE_DIR, \"class_related_keywords.txt\")\n",
    "TRAIN_PATH     = os.path.join(BASE_DIR, \"train\", \"train_corpus.txt\")\n",
    "TEST_PATH      = os.path.join(BASE_DIR, \"test\", \"test_corpus.txt\")\n",
    "\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "NUM_CLASSES = 531\n",
    "MIN_LABELS = 2\n",
    "MAX_LABELS = 3\n",
    "\n",
    "# Sentence-BERT model (빠르고 성능 괜찮은 기본값)\n",
    "SBERT_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE_ENC = 64        # GPU면 128~256 가능\n",
    "MAX_CHARS_DOC = 1500       # 너무 긴 문서 자르기 (속도/노이즈)\n",
    "DEPTH_ALPHA = 0.03         # \"더 구체(깊은) 클래스\"를 살짝 선호하는 가중치\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# I/O\n",
    "# ------------------------\n",
    "def load_corpus(path):\n",
    "    pids, texts = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pids.append(pid)\n",
    "                texts.append(text)\n",
    "    return pids, texts\n",
    "\n",
    "def load_classes(path):\n",
    "    id2name = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            parts = re.split(r\"[\\t,]\", s, maxsplit=1)\n",
    "            if len(parts) == 2 and parts[0].strip().isdigit():\n",
    "                cid = int(parts[0].strip())\n",
    "                id2name[cid] = parts[1].strip()\n",
    "            else:\n",
    "                id2name[i] = s\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        if cid not in id2name:\n",
    "            id2name[cid] = f\"class_{cid}\"\n",
    "    return id2name\n",
    "\n",
    "def load_keywords_accumulate(path):\n",
    "    kw = {cid: [] for cid in range(NUM_CLASSES)}\n",
    "\n",
    "    def split_tokens(fields):\n",
    "        out = []\n",
    "        for r in fields:\n",
    "            out.extend([x.strip() for x in re.split(r\"[,;/|]\", r) if x.strip()])\n",
    "        # 숫자 토큰 제거(가중치가 섞여있는 경우가 많음)\n",
    "        out = [x for x in out if not re.fullmatch(r\"[-+]?\\d+(\\.\\d+)?\", x)]\n",
    "        return out\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            parts = s.split(\"\\t\")\n",
    "            if len(parts) == 1:\n",
    "                parts = s.split(\",\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            cid_raw = parts[0].strip()\n",
    "            if not cid_raw.isdigit():\n",
    "                continue\n",
    "            cid = int(cid_raw)\n",
    "            if cid < 0 or cid >= NUM_CLASSES:\n",
    "                continue\n",
    "            rest = [p.strip() for p in parts[1:] if p.strip()]\n",
    "            toks = split_tokens(rest)\n",
    "\n",
    "            seen = set(kw[cid])\n",
    "            for t in toks:\n",
    "                if t not in seen:\n",
    "                    kw[cid].append(t)\n",
    "                    seen.add(t)\n",
    "    return kw\n",
    "\n",
    "def read_edges(path, id2name):\n",
    "    name2id = {v: k for k, v in id2name.items()}\n",
    "\n",
    "    def to_id(x):\n",
    "        x = x.strip()\n",
    "        if x.isdigit():\n",
    "            return int(x)\n",
    "        return name2id.get(x, None)\n",
    "\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            parts = re.split(r\"[\\t,]\", s)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            a = to_id(parts[0]); b = to_id(parts[1])\n",
    "            if a is None or b is None or a == b:\n",
    "                continue\n",
    "            if 0 <= a < NUM_CLASSES and 0 <= b < NUM_CLASSES:\n",
    "                edges.append((a, b))\n",
    "    return edges\n",
    "\n",
    "def build_graph(edges, parent_to_child=True):\n",
    "    parents_of = {i: set() for i in range(NUM_CLASSES)}\n",
    "    children_of = {i: set() for i in range(NUM_CLASSES)}\n",
    "\n",
    "    for a, b in edges:\n",
    "        if parent_to_child:\n",
    "            p, c = a, b\n",
    "        else:\n",
    "            p, c = b, a\n",
    "        if p != c:\n",
    "            parents_of[c].add(p)\n",
    "            children_of[p].add(c)\n",
    "\n",
    "    roots = [i for i in range(NUM_CLASSES) if len(parents_of[i]) == 0]\n",
    "    parents_of = {k: sorted(v) for k, v in parents_of.items()}\n",
    "    children_of = {k: sorted(v) for k, v in children_of.items()}\n",
    "    return roots, parents_of, children_of\n",
    "\n",
    "def load_hierarchy_autodetect(path, id2name):\n",
    "    edges = read_edges(path, id2name)\n",
    "    r1, p1, c1 = build_graph(edges, parent_to_child=True)\n",
    "    r2, p2, c2 = build_graph(edges, parent_to_child=False)\n",
    "    # 보통 taxonomy는 root 수가 작습니다. root가 더 적은 방향을 선택\n",
    "    if 1 <= len(r2) < len(r1):\n",
    "        return r2, p2, c2, \"inverted(child->parent in file)\"\n",
    "    return r1, p1, c1, \"as-is(parent->child in file)\"\n",
    "\n",
    "def compute_depths(roots, children_of):\n",
    "    # BFS로 최소 depth 계산\n",
    "    depth = np.full(NUM_CLASSES, 10**9, dtype=np.int32)\n",
    "    from collections import deque\n",
    "    q = deque()\n",
    "    for r in roots:\n",
    "        depth[r] = 0\n",
    "        q.append(r)\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        for v in children_of.get(u, []):\n",
    "            if depth[v] > depth[u] + 1:\n",
    "                depth[v] = depth[u] + 1\n",
    "                q.append(v)\n",
    "    # 도달 불가 노드가 있으면 큰 값이 유지되므로 정리\n",
    "    maxd = int(np.max(depth[depth < 10**9])) if np.any(depth < 10**9) else 0\n",
    "    depth[depth >= 10**9] = maxd + 1\n",
    "    return depth\n",
    "\n",
    "def get_ancestors(cid, parents_of, max_steps=10):\n",
    "    anc = []\n",
    "    cur = cid\n",
    "    for _ in range(max_steps):\n",
    "        plist = parents_of.get(cur, [])\n",
    "        if not plist:\n",
    "            break\n",
    "        p = plist[0]  # deterministic: smallest parent\n",
    "        anc.append(p)\n",
    "        cur = p\n",
    "    return anc\n",
    "\n",
    "def cores_to_labels(cores, parents_of):\n",
    "    labels = []\n",
    "    for c in cores:\n",
    "        labels.append(c)\n",
    "    for c in cores:\n",
    "        for a in get_ancestors(c, parents_of, max_steps=5):\n",
    "            if len(labels) >= MAX_LABELS:\n",
    "                break\n",
    "            labels.append(a)\n",
    "        if len(labels) >= MAX_LABELS:\n",
    "            break\n",
    "\n",
    "    # dedup keep order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for x in labels:\n",
    "        if x not in seen:\n",
    "            uniq.append(x); seen.add(x)\n",
    "    labels = uniq\n",
    "\n",
    "    # ensure 2~3\n",
    "    if len(labels) < MIN_LABELS and cores:\n",
    "        for a in get_ancestors(cores[0], parents_of, max_steps=10):\n",
    "            if a not in seen:\n",
    "                labels.append(a); seen.add(a)\n",
    "            if len(labels) >= MIN_LABELS:\n",
    "                break\n",
    "\n",
    "    if len(labels) < MIN_LABELS:\n",
    "        for k in range(NUM_CLASSES):\n",
    "            if k not in seen:\n",
    "                labels.append(k); seen.add(k)\n",
    "            if len(labels) >= MIN_LABELS:\n",
    "                break\n",
    "\n",
    "    labels = labels[:MAX_LABELS]\n",
    "    return sorted(labels)\n",
    "\n",
    "def build_class_texts(id2name, kw):\n",
    "    # 클래스 텍스트를 조금 더 \"문장\"처럼 만들어 의미 임베딩이 잘 먹도록 함\n",
    "    texts = []\n",
    "    for cid in range(NUM_CLASSES):\n",
    "        name = id2name[cid]\n",
    "        kws = kw.get(cid, [])[:40]\n",
    "        # 템플릿: NLI는 아니지만 SBERT에서 의미 매칭이 좋아지는 경우가 많음\n",
    "        t = f\"This document is about {name}. Keywords: \" + \", \".join(kws) if kws else f\"This document is about {name}.\"\n",
    "        texts.append(t)\n",
    "    return texts\n",
    "\n",
    "def l2_normalize(x, eps=1e-12):\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    return x / np.maximum(n, eps)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Core mining using embeddings\n",
    "# ------------------------\n",
    "def mine_cores_with_embeddings(doc_emb, class_emb, roots, parents_of, children_of, depth):\n",
    "    \"\"\"\n",
    "    doc_emb: (N,D) normalized\n",
    "    class_emb: (C,D) normalized\n",
    "    Return: doc_cores list[list[int]]\n",
    "    \"\"\"\n",
    "    root_set = set(roots)\n",
    "\n",
    "    # median conf per class를 계산하면 또다시 붕괴할 수 있어,\n",
    "    # 여기서는 \"conf>0\" + \"top candidates\" + \"fallback\"으로 안정적으로 운영\n",
    "    doc_cores = []\n",
    "\n",
    "    for i in tqdm(range(doc_emb.shape[0]), desc=\"Core mining (embedding)\"):\n",
    "        sim = doc_emb[i] @ class_emb.T  # (C,)\n",
    "\n",
    "        # 후보: 전체 중 상위 M개만 고려 (전체를 다 쓰면 상위 클래스가 너무 유리해질 수 있음)\n",
    "        M = 60\n",
    "        top_idx = np.argpartition(-sim, M)[:M]\n",
    "        # 깊이 가중(구체 클래스 선호): sim' = sim + alpha*depth\n",
    "        sim2 = sim[top_idx] + DEPTH_ALPHA * depth[top_idx].astype(np.float32)\n",
    "        # sim2 기준 정렬\n",
    "        order = top_idx[np.argsort(-sim2)]\n",
    "\n",
    "        # confidence 계산(부모/형제 대비)\n",
    "        cand = []\n",
    "        for c in order:\n",
    "            if c in root_set:\n",
    "                continue\n",
    "            plist = parents_of.get(c, [])\n",
    "            if not plist:\n",
    "                continue\n",
    "            p = plist[0]\n",
    "            parent_sim = sim[p]\n",
    "\n",
    "            sib_max = -1e9\n",
    "            for s in children_of.get(p, []):\n",
    "                if s == c:\n",
    "                    continue\n",
    "                sib_max = max(sib_max, sim[s])\n",
    "\n",
    "            conf = float(sim[c] - max(parent_sim, sib_max))\n",
    "            cand.append((c, conf, float(sim[c]), int(depth[c])))\n",
    "\n",
    "        # core 선택: conf가 양수이고, conf 상위\n",
    "        cand_pos = [x for x in cand if x[1] > 0]\n",
    "        cand_pos.sort(key=lambda x: (x[1], x[3]), reverse=True)  # conf 우선, 깊이 보조\n",
    "\n",
    "        if cand_pos:\n",
    "            cores = [cand_pos[0][0]]\n",
    "            # 두 번째 core는 \"충분히 유사\"하고 conf도 괜찮으면 추가\n",
    "            for x in cand_pos[1:]:\n",
    "                if x[2] >= 0.92 * cand_pos[0][2]:\n",
    "                    cores.append(x[0])\n",
    "                    break\n",
    "            doc_cores.append(cores[:2])\n",
    "        else:\n",
    "            # fallback: (루트 제외) sim+depth 가중치 최고 1개\n",
    "            best = None\n",
    "            best_score = -1e9\n",
    "            for c in order:\n",
    "                if c in root_set:\n",
    "                    continue\n",
    "                score = float(sim[c] + DEPTH_ALPHA * depth[c])\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best = c\n",
    "            doc_cores.append([best] if best is not None else [0])\n",
    "\n",
    "    return doc_cores\n",
    "\n",
    "\n",
    "def main():\n",
    "    for p in [CLASSES_PATH, HIER_PATH, KEYWORDS_PATH, TRAIN_PATH, TEST_PATH]:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "    # Load resources\n",
    "    id2name = load_classes(CLASSES_PATH)\n",
    "    kw = load_keywords_accumulate(KEYWORDS_PATH)\n",
    "    roots, parents_of, children_of, note = load_hierarchy_autodetect(HIER_PATH, id2name)\n",
    "\n",
    "    print(f\"[INFO] taxonomy direction: {note}\")\n",
    "    print(f\"[INFO] roots: {len(roots)}\")\n",
    "\n",
    "    depth = compute_depths(roots, children_of)\n",
    "\n",
    "    train_pids, train_texts = load_corpus(TRAIN_PATH)\n",
    "    test_pids, test_texts = load_corpus(TEST_PATH)\n",
    "\n",
    "    # shorten docs to reduce noise / speed up\n",
    "    train_texts = [t[:MAX_CHARS_DOC] for t in train_texts]\n",
    "    test_texts  = [t[:MAX_CHARS_DOC] for t in test_texts]\n",
    "\n",
    "    class_texts = build_class_texts(id2name, kw)\n",
    "\n",
    "    # Encode with SentenceTransformer\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\n",
    "            \"sentence-transformers is not installed. Run:\\n\"\n",
    "            \"  pip install -q sentence-transformers\\n\"\n",
    "            \"and re-run.\"\n",
    "        ) from e\n",
    "\n",
    "    model = SentenceTransformer(SBERT_NAME)\n",
    "\n",
    "    print(\"[INFO] Encoding class texts...\")\n",
    "    class_emb = model.encode(\n",
    "        class_texts,\n",
    "        batch_size=BATCH_SIZE_ENC,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Encoding train docs...\")\n",
    "    train_emb = model.encode(\n",
    "        train_texts,\n",
    "        batch_size=BATCH_SIZE_ENC,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Encoding test docs...\")\n",
    "    test_emb = model.encode(\n",
    "        test_texts,\n",
    "        batch_size=BATCH_SIZE_ENC,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "\n",
    "    # --- Core mining on TEST directly (zero-shot style) ---\n",
    "    # 가장 먼저 \"붕괴가 멈추는지\" 확인하려면 이 방식이 가장 안전합니다.\n",
    "    test_cores = mine_cores_with_embeddings(\n",
    "        doc_emb=test_emb,\n",
    "        class_emb=class_emb,\n",
    "        roots=roots,\n",
    "        parents_of=parents_of,\n",
    "        children_of=children_of,\n",
    "        depth=depth\n",
    "    )\n",
    "\n",
    "    # Build final labels (2~3)\n",
    "    test_labels = [cores_to_labels(cores, parents_of) for cores in test_cores]\n",
    "\n",
    "    # Diagnostics: top patterns\n",
    "    from collections import Counter\n",
    "    freq = Counter(tuple(x) for x in test_labels)\n",
    "    print(\"[INFO] top-10 predicted label patterns:\", freq.most_common(10))\n",
    "\n",
    "    # Write submission\n",
    "    with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"id\", \"labels\"])\n",
    "        for pid, labs in zip(test_pids, test_labels):\n",
    "            w.writerow([pid, \",\".join(map(str, labs))])\n",
    "\n",
    "    print(f\"[DONE] saved: {SUBMISSION_PATH}\")\n",
    "    print(f\"[DONE] test samples: {len(test_pids)} | labels per sample: {MIN_LABELS}-{MAX_LABELS}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
