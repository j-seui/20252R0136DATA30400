{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# =========================\n",
    "# User Config\n",
    "# =========================\n",
    "API_KEY = \"AQ.Ab8RN6JUb82VrSIvjvHxX1wie7AH4c-SzUSd6pGwiHcCbRUt3g\"\n",
    "\n",
    "GEMINI_MODEL = \"gemini-2.0-flash-lite-001\"\n",
    "\n",
    "# Data Paths\n",
    "ROOT_DIR = \"Amazon_products\"\n",
    "CLASSES_PATH = os.path.join(ROOT_DIR, \"classes.txt\")\n",
    "HIER_PATH = os.path.join(ROOT_DIR, \"class_hierarchy.txt\")\n",
    "KW_PATH = os.path.join(ROOT_DIR, \"class_related_keywords.txt\")\n",
    "TRAIN_CORPUS_PATH = os.path.join(ROOT_DIR, \"train\", \"train_corpus.txt\")\n",
    "TEST_CORPUS_PATH = os.path.join(ROOT_DIR, \"test\", \"test_corpus.txt\")\n",
    "\n",
    "# Auto Save\n",
    "ARTIFACT_DIR = os.path.join(ROOT_DIR, \"_artifacts_roberta\")\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Auto Save Paths (중간에 정지해도 이어서 할 수 있도록)\n",
    "CLASS_DESC_JSON = os.path.join(ARTIFACT_DIR, \"class_descriptions.json\")\n",
    "CLASS_EMB_NPY = os.path.join(ARTIFACT_DIR, \"class_embeddings.npy\")\n",
    "REVIEW_EMB_NPY = os.path.join(ARTIFACT_DIR, \"review_embeddings.npy\")\n",
    "\n",
    "CORE_LEAF_NPY = os.path.join(ARTIFACT_DIR, \"core_leaf.npy\")\n",
    "CORE_LEAF_SOURCE_NPY = os.path.join(ARTIFACT_DIR, \"core_leaf_src.npy\")\n",
    "CORE_LEAF_META_JSON = os.path.join(ARTIFACT_DIR, \"core_leaf_meta.json\")\n",
    "\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "# API usage guard\n",
    "API_CALL_LIMIT = 1000\n",
    "API_CALL_COUNT = 0\n",
    "\n",
    "# Embedding model\n",
    "EMB_MODEL_NAME = \"princeton-nlp/sup-simcse-roberta-large\"\n",
    "MAX_SEQ_LEN = 256\n",
    "EMB_BATCH_SIZE = 64\n",
    "\n",
    "# LLM fallback batching\n",
    "TOPK_CANDIDATES_FOR_LLM = 5\n",
    "LLM_BATCH_SIZE = 10\n",
    "\n",
    "# Threshold calibration\n",
    "PERCENTILE_FOR_THRESHOLD = 15 # 15 percentile\n",
    "THRESHOLD_FLOOR = 0.20\n",
    "\n",
    "# Parent greedy selection\n",
    "PARENT_MIN_SIM = 0.05\n",
    "DROP_ABS_DELTA = 0.25      # 절대 감소\n",
    "DROP_REL_RATIO = 0.60      # 상대 감소(60% 이상 감소면 급감으로 판단)\n",
    "\n",
    "# Self-training hyperparams\n",
    "SELF_TRAIN_NUM_ITERS = 2\n",
    "SELF_TRAIN_EPOCHS_PER_ITER = 4\n",
    "SELF_TRAIN_LR = 2e-3\n",
    "SELF_TRAIN_BATCH_SIZE = 256\n",
    "SELF_TRAIN_WEIGHT_DECAY = 0.01\n",
    "SELF_TRAIN_CONF_THRESHOLD = 0.90     # pseudo label 확신도\n",
    "SELF_TRAIN_MAX_NEW_PER_ITER = 20000  # 한 iter에서 새로 편입할 최대 개수\n",
    "SELF_TRAIN_SEED_MIN_SIM = None       # None이면 core threshold(thr) 사용\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Reproducibility\n",
    "# =========================\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def is_int_str(x: str) -> bool:\n",
    "    try:\n",
    "        int(x)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def safe_json_load(path: str, default=None):\n",
    "    if default is None:\n",
    "        default = {}\n",
    "    if not os.path.exists(path):\n",
    "        return default\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def safe_json_save(obj: Any, path: str):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "\n",
    "def normalize_rows(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    return x / np.maximum(n, eps)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Data Loading\n",
    "# =========================\n",
    "def load_classes(path: str) -> Tuple[Dict[int, str], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    classes.txt: \"class_id<TAB>class_name\" 또는 \"class_id class_name\"\n",
    "    \"\"\"\n",
    "    id2name = {}\n",
    "    name2id = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ln = line.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            parts = re.split(r\"[\\t ]+\", ln, maxsplit=1)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            cid, cname = parts[0], parts[1].strip()\n",
    "            if not is_int_str(cid):\n",
    "                continue\n",
    "            cid = int(cid)\n",
    "            id2name[cid] = cname\n",
    "            name2id[cname] = cid\n",
    "    return id2name, name2id\n",
    "\n",
    "\n",
    "def load_keywords(path: str) -> Dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    class_related_keywords.txt: \"class_id<TAB>kw1,kw2,...\"\n",
    "    \"\"\"\n",
    "    cid2kws = defaultdict(list)\n",
    "    if not os.path.exists(path):\n",
    "        return dict(cid2kws)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ln = line.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            parts = ln.split(\"\\t\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            cid = parts[0]\n",
    "            if not is_int_str(cid):\n",
    "                continue\n",
    "            cid = int(cid)\n",
    "            kws = re.split(r\"[,\\|;]+\", parts[1])\n",
    "            kws = [k.strip() for k in kws if k.strip()]\n",
    "            cid2kws[cid].extend(kws)\n",
    "    return dict(cid2kws)\n",
    "\n",
    "\n",
    "def load_corpus_any(path: str, split_name: str):\n",
    "    \"\"\"\n",
    "    train_corpus.txt / test_corpus.txt:\n",
    "      - 최소: id<TAB>text\n",
    "      - 혹시 label 컬럼이 있더라도: 마지막 컬럼을 text로 사용\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "            elif len(parts) >= 3:\n",
    "                pid = parts[0]\n",
    "                text = parts[-1]\n",
    "            else:\n",
    "                continue\n",
    "            rows.append({\"split\": split_name, \"pid\": pid, \"text\": text})\n",
    "    return rows\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DAG Hierarchy (다중 parent 허용)\n",
    "# (변수명은 parent_of / children_of 그대로 사용하되,\n",
    "#  parent_of[child] = set(parents) 로 의미만 DAG로 확장)\n",
    "# =========================\n",
    "def load_hierarchy(path: str, num_classes: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    class_hierarchy.txt에서 (parent, child) 페어들을 읽고,\n",
    "    DAG로 로딩 (다중 parent 허용).\n",
    "    - parent_of: child -> set(parents)\n",
    "    - children_of: parent -> set(children)\n",
    "    사이클이 있으면 오류.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "    pairs = []\n",
    "    max_id = -1\n",
    "    for line in lines:\n",
    "        parts = re.split(r\"[\\t, ]+\", line)\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        a, b = parts[0], parts[1]\n",
    "        if is_int_str(a) and is_int_str(b):\n",
    "            p = int(a)\n",
    "            c = int(b)\n",
    "            pairs.append((p, c))\n",
    "            max_id = max(max_id, p, c)\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_nodes = max_id + 1\n",
    "    else:\n",
    "        num_nodes = num_classes\n",
    "\n",
    "    parent_of = defaultdict(set)   # child -> {parents}\n",
    "    children_of = defaultdict(set) # parent -> {children}\n",
    "\n",
    "    for p, c in pairs:\n",
    "        if p == c:\n",
    "            continue\n",
    "        parent_of[c].add(p)\n",
    "        children_of[p].add(c)\n",
    "\n",
    "    # Kahn's algorithm for cycle check (DAG 검증)\n",
    "    indeg = [0] * num_nodes\n",
    "    for node in range(num_nodes):\n",
    "        indeg[node] = len(parent_of.get(node, set()))\n",
    "\n",
    "    q = deque([i for i in range(num_nodes) if indeg[i] == 0])\n",
    "    visited = 0\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        visited += 1\n",
    "        for v in children_of.get(u, set()):\n",
    "            indeg[v] -= 1\n",
    "            if indeg[v] == 0:\n",
    "                q.append(v)\n",
    "\n",
    "    if visited != num_nodes:\n",
    "        raise ValueError(\"Hierarchy has a cycle (not a DAG). Please check class_hierarchy.txt\")\n",
    "\n",
    "    return parent_of, children_of\n",
    "\n",
    "\n",
    "def get_leaf_nodes(children_of: Dict[int, set], num_classes: int) -> List[int]:\n",
    "    leaves = []\n",
    "    for cid in range(num_classes):\n",
    "        if len(children_of.get(cid, set())) == 0:\n",
    "            leaves.append(cid)\n",
    "    return leaves\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Using API Key to call Vertex AI Gemini\n",
    "# =========================\n",
    "def vertex_generate_content(prompt: str, temperature: float = 0.2, max_output_tokens: int = 2048) -> str:\n",
    "    \"\"\"\n",
    "    Express mode non-stream endpoint:\n",
    "      POST https://aiplatform.googleapis.com/v1/publishers/google/models/{model}:generateContent?key=API_KEY\n",
    "    \"\"\"\n",
    "    global API_CALL_COUNT\n",
    "    if API_CALL_COUNT >= API_CALL_LIMIT:\n",
    "        raise RuntimeError(f\"API_CALL_LIMIT exceeded ({API_CALL_LIMIT}).\")\n",
    "\n",
    "    url = (\n",
    "        f\"https://aiplatform.googleapis.com/v1/publishers/google/models/{GEMINI_MODEL}:generateContent\"\n",
    "        f\"?key={API_KEY}\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"contents\": [\n",
    "            {\"role\": \"user\", \"parts\": [{\"text\": prompt}]}\n",
    "        ],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": float(temperature),\n",
    "            \"maxOutputTokens\": int(max_output_tokens),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 간단 retry\n",
    "    last_err = None\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            API_CALL_COUNT += 1\n",
    "            resp = requests.post(url, json=payload, timeout=120)\n",
    "            if resp.status_code != 200:\n",
    "                last_err = RuntimeError(f\"HTTP {resp.status_code}: {resp.text[:500]}\")\n",
    "                time.sleep(1.5 * (attempt + 1))\n",
    "                continue\n",
    "            data = resp.json()\n",
    "            return _extract_text_from_generatecontent(data)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(1.5 * (attempt + 1))\n",
    "    raise RuntimeError(f\"vertex_generate_content failed: {last_err}\")\n",
    "\n",
    "\n",
    "def _extract_text_from_generatecontent(data: dict) -> str:\n",
    "    \"\"\"\n",
    "    generateContent 응답에서 텍스트를 최대한 안정적으로 추출\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cands = data.get(\"candidates\", [])\n",
    "        if not cands:\n",
    "            return \"\"\n",
    "        parts = cands[0].get(\"content\", {}).get(\"parts\", [])\n",
    "        texts = []\n",
    "        for p in parts:\n",
    "            t = p.get(\"text\")\n",
    "            if isinstance(t, str):\n",
    "                texts.append(t)\n",
    "        return \"\\n\".join(texts).strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Class description generation (cached)\n",
    "# =========================\n",
    "def build_desc_prompt(cid: int, cname: str, kws: List[str]) -> str:\n",
    "    kw_str = \", \".join(kws[:30]) if kws else \"\"\n",
    "    return (\n",
    "        \"You are given a product review classification label.\\n\"\n",
    "        \"Write a short, concrete description of what kinds of products/reviews belong to this label.\\n\"\n",
    "        \"Keep it concise but specific.\\n\\n\"\n",
    "        f\"Label ID: {cid}\\n\"\n",
    "        f\"Label Name: {cname}\\n\"\n",
    "        f\"Related keywords: {kw_str}\\n\\n\"\n",
    "        \"Return ONLY the description (one paragraph).\"\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_desc_response(text: str) -> str:\n",
    "    # 과도한 포맷 제거\n",
    "    t = text.strip()\n",
    "    t = re.sub(r\"^\\s*[-*]\\s*\", \"\", t)\n",
    "    t = t.strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def generate_class_descriptions(id2name: Dict[int, str], cid2kws: Dict[int, List[str]]) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    CLASS_DESC_JSON을 캐시로 사용:\n",
    "    - 이미 존재하는 cid는 재호출하지 않음\n",
    "    \"\"\"\n",
    "    desc_map = safe_json_load(CLASS_DESC_JSON, default={})\n",
    "    # json 키는 str일 수 있으니 int로 normalize\n",
    "    norm_map = {}\n",
    "    for k, v in desc_map.items():\n",
    "        if is_int_str(str(k)):\n",
    "            norm_map[int(k)] = str(v)\n",
    "    desc_map = norm_map\n",
    "\n",
    "    missing = [cid for cid in id2name.keys() if cid not in desc_map or not str(desc_map[cid]).strip()]\n",
    "    if not missing:\n",
    "        return desc_map\n",
    "\n",
    "    print(f\"[Class desc] cached={len(desc_map)}, missing={len(missing)} (will call LLM)\")\n",
    "    for cid in tqdm(missing, desc=\"Generate class descriptions\"):\n",
    "        cname = id2name[cid]\n",
    "        kws = cid2kws.get(cid, [])\n",
    "        prompt = build_desc_prompt(cid, cname, kws)\n",
    "        out = vertex_generate_content(prompt, temperature=0.2, max_output_tokens=512)\n",
    "        desc = parse_desc_response(out)\n",
    "        if not desc:\n",
    "            desc = cname\n",
    "        desc_map[cid] = desc\n",
    "        # class 단위로 저장(중단/재개 가능)\n",
    "        safe_json_save({str(k): v for k, v in desc_map.items()}, CLASS_DESC_JSON)\n",
    "\n",
    "    return desc_map\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Embedding (cached)\n",
    "# =========================\n",
    "\n",
    "# Mean Pooling (하나의 벡터로)\n",
    "def mean_pool(last_hidden: torch.Tensor, attn_mask: torch.Tensor) -> torch.Tensor:\n",
    "    # last_hidden: [B, T, H], attn_mask: [B, T]\n",
    "    mask = attn_mask.unsqueeze(-1).type_as(last_hidden)  # [B, T, 1]\n",
    "    summed = (last_hidden * mask).sum(dim=1)\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "    return summed / denom\n",
    "\n",
    "# 텍스트 임베딩 생성\n",
    "def embed_texts_roberta(texts: List[str], device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> np.ndarray:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMB_MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(EMB_MODEL_NAME).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    outs = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), EMB_BATCH_SIZE), desc=f\"Embedding ({EMB_MODEL_NAME})\"):\n",
    "            batch = texts[i:i + EMB_BATCH_SIZE]\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "            out = model(**enc)\n",
    "            pooled = mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "            pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "            outs.append(pooled.detach().cpu().numpy())\n",
    "    return np.vstack(outs).astype(np.float32)\n",
    "\n",
    "\n",
    "# 임베딩 생성 (캐시 사용)   \n",
    "def build_or_load_embeddings(\n",
    "    class_desc_map: Dict[int, str],\n",
    "    all_rows: List[dict],\n",
    "    num_classes: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    CLASS_EMB_NPY, REVIEW_EMB_NPY 캐시 활용\n",
    "    \"\"\"\n",
    "    # 1) class embeddings\n",
    "    if os.path.exists(CLASS_EMB_NPY):\n",
    "        class_emb = np.load(CLASS_EMB_NPY)\n",
    "    else:\n",
    "        class_texts = []\n",
    "        for cid in range(num_classes):\n",
    "            class_texts.append(str(class_desc_map.get(cid, \"\")))\n",
    "        class_emb = embed_texts_roberta(class_texts)\n",
    "        np.save(CLASS_EMB_NPY, class_emb)\n",
    "\n",
    "    # 2) review embeddings\n",
    "    if os.path.exists(REVIEW_EMB_NPY):\n",
    "        review_emb = np.load(REVIEW_EMB_NPY)\n",
    "    else:\n",
    "        review_texts = [r[\"text\"] for r in all_rows]\n",
    "        review_emb = embed_texts_roberta(review_texts)\n",
    "        np.save(REVIEW_EMB_NPY, review_emb)\n",
    "\n",
    "    # 정규화(안전)\n",
    "    class_emb = normalize_rows(class_emb.astype(np.float32))\n",
    "    review_emb = normalize_rows(review_emb.astype(np.float32))\n",
    "    return class_emb, review_emb\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Core leaf inference (cosine + LLM fallback) with checkpoint\n",
    "# =========================\n",
    "\n",
    "# similarity 기반 threshold 산출\n",
    "def calibrate_threshold(leaf_ids: List[int], leaf_emb: np.ndarray, review_emb_all: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    leaf top-1 cosine maxsim 분포에서 percentile 기반 threshold 산출\n",
    "    \"\"\"\n",
    "    leaf_t = leaf_emb.T\n",
    "    maxs = []\n",
    "    bs = 2048\n",
    "    for i in range(0, review_emb_all.shape[0], bs):\n",
    "        chunk = review_emb_all[i:i + bs]\n",
    "        sims = chunk @ leaf_t\n",
    "        maxs.append(sims.max(axis=1))\n",
    "    maxs = np.concatenate(maxs, axis=0)\n",
    "    thr = float(np.percentile(maxs, PERCENTILE_FOR_THRESHOLD))\n",
    "    thr = max(thr, THRESHOLD_FLOOR)\n",
    "    return float(thr)\n",
    "\n",
    "# LLM 배치 프롬프트 생성\n",
    "def build_llm_batch_prompt(batch_items: List[Tuple[int, str, List[Tuple[int, str]]]]) -> str:\n",
    "    \"\"\"\n",
    "    batch_items: [(review_idx, review_text, [(cid, cname), ...topk])]\n",
    "    LLM에게 각 review_idx별로 cid 하나를 선택하게 함\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"You are a classifier. For each item, choose the best label ID from the candidates.\\n\")\n",
    "    lines.append(\"Return STRICT JSON only: a list of objects with keys {idx, cid}.\\n\")\n",
    "    lines.append(\"Do not include any extra text.\\n\\n\")\n",
    "\n",
    "    for idx, text, cands in batch_items:\n",
    "        cand_str = \"; \".join([f\"{cid}:{cname}\" for cid, cname in cands])\n",
    "        lines.append(f\"idx={idx}\\nreview={text}\\ncandidates={cand_str}\\n\")\n",
    "\n",
    "    lines.append(\"\\nJSON format example: [{\\\"idx\\\": 12, \\\"cid\\\": 5}, ...]\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# LLM 응답 파싱\n",
    "def parse_llm_choice_response(text: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    LLM 응답에서 (idx, cid) 리스트를 복구\n",
    "    \"\"\"\n",
    "    t = text.strip()\n",
    "    # JSON 블록만 남기기 시도\n",
    "    m = re.search(r\"\\[.*\\]\", t, flags=re.S)\n",
    "    if m:\n",
    "        t = m.group(0)\n",
    "    try:\n",
    "        data = json.loads(t)\n",
    "        out = []\n",
    "        if isinstance(data, list):\n",
    "            for it in data:\n",
    "                if isinstance(it, dict) and \"idx\" in it and \"cid\" in it:\n",
    "                    out.append((int(it[\"idx\"]), int(it[\"cid\"])))\n",
    "        return out\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "# 코어 리프 클래스 추론\n",
    "def infer_core_leaf_classes(\n",
    "    leaf_ids, leaf_emb, review_emb_all, all_reviews, id2name, class_desc_map\n",
    "):\n",
    "    \"\"\"\n",
    "    체크포인트:\n",
    "      - CORE_LEAF_NPY: 리뷰 idx -> core leaf class_id\n",
    "      - CORE_LEAF_SOURCE_NPY: 0=cosine, 1=llm\n",
    "      - CORE_LEAF_META_JSON: threshold 및 leaf_ids 검증\n",
    "    중단되더라도 저장되므로 재실행 시 이어서 진행.\n",
    "    \"\"\"\n",
    "    global API_CALL_COUNT\n",
    "\n",
    "    leaf_ids = list(leaf_ids)\n",
    "    leaf_t = leaf_emb.T\n",
    "    n = review_emb_all.shape[0]\n",
    "\n",
    "    # 1) threshold\n",
    "    thr = calibrate_threshold(leaf_ids, leaf_emb, review_emb_all)\n",
    "\n",
    "    # 2) load or init checkpoint\n",
    "    ok = False\n",
    "    if os.path.exists(CORE_LEAF_NPY) and os.path.exists(CORE_LEAF_SOURCE_NPY) and os.path.exists(CORE_LEAF_META_JSON):\n",
    "        meta = safe_json_load(CORE_LEAF_META_JSON, default={})\n",
    "        if int(meta.get(\"leaf_count\", -1)) == len(leaf_ids) and float(meta.get(\"threshold\", -1.0)) == float(thr):\n",
    "            ok = True\n",
    "\n",
    "    if ok:\n",
    "        core_leaf = np.load(CORE_LEAF_NPY).astype(np.int32)\n",
    "        core_src = np.load(CORE_LEAF_SOURCE_NPY).astype(np.uint8)\n",
    "        if len(core_leaf) != n:\n",
    "            ok = False\n",
    "\n",
    "    if not ok:\n",
    "        core_leaf = np.empty(n, dtype=np.int32)\n",
    "        core_src = np.zeros(n, dtype=np.uint8)  # 0=cosine default\n",
    "\n",
    "        bs = 2048\n",
    "        for i in tqdm(range(0, n, bs), desc=\"(Checkpoint) cosine-top1 core leaf\"):\n",
    "            chunk = review_emb_all[i:i + bs]\n",
    "            sims = chunk @ leaf_t\n",
    "            arg = sims.argmax(axis=1)\n",
    "            core_leaf[i:i + bs] = np.array([leaf_ids[int(p)] for p in arg], dtype=np.int32)\n",
    "\n",
    "        np.save(CORE_LEAF_NPY, core_leaf)\n",
    "        np.save(CORE_LEAF_SOURCE_NPY, core_src)\n",
    "        safe_json_save({\"threshold\": thr, \"leaf_count\": len(leaf_ids)}, CORE_LEAF_META_JSON)\n",
    "\n",
    "    # 3) LLM 보정: maxsim < thr 인 리뷰만\n",
    "    ambiguous_idx = []\n",
    "    bs = 2048\n",
    "    for i in range(0, n, bs):\n",
    "        chunk = review_emb_all[i:i + bs]\n",
    "        sims = chunk @ leaf_t\n",
    "        maxsim = sims.max(axis=1)\n",
    "        for j, s in enumerate(maxsim):\n",
    "            idx = i + j\n",
    "            if float(s) < float(thr) and core_src[idx] != 1:\n",
    "                ambiguous_idx.append(idx)\n",
    "\n",
    "    print(f\"[Ambiguous remaining] {len(ambiguous_idx)} items require LLM (batched {LLM_BATCH_SIZE}).\")\n",
    "    if not ambiguous_idx:\n",
    "        return {i: int(core_leaf[i]) for i in range(n)}\n",
    "\n",
    "    # LLM 배치 처리(중단/재개: core_src==1은 skip)\n",
    "    for st in tqdm(range(0, len(ambiguous_idx), LLM_BATCH_SIZE), desc=\"LLM disambiguation\"):\n",
    "        batch = ambiguous_idx[st:st + LLM_BATCH_SIZE]\n",
    "        batch_items = []\n",
    "        for idx in batch:\n",
    "            text = all_reviews[idx][\"text\"]\n",
    "            # topk candidates by cosine\n",
    "            sims = review_emb_all[idx] @ leaf_t\n",
    "            topk = np.argsort(-sims)[:TOPK_CANDIDATES_FOR_LLM]\n",
    "            cands = [(leaf_ids[int(j)], id2name.get(int(leaf_ids[int(j)]), str(leaf_ids[int(j)]))) for j in topk]\n",
    "            batch_items.append((idx, text, cands))\n",
    "\n",
    "        prompt = build_llm_batch_prompt(batch_items)\n",
    "        resp = vertex_generate_content(prompt, temperature=0.0, max_output_tokens=1024)\n",
    "        choices = parse_llm_choice_response(resp)\n",
    "\n",
    "        # 적용\n",
    "        for i2, cid in choices:\n",
    "            i2 = int(i2)\n",
    "            if 0 <= i2 < n:\n",
    "                core_leaf[i2] = int(cid)\n",
    "                core_src[i2] = 1\n",
    "\n",
    "        # checkpoint\n",
    "        np.save(CORE_LEAF_NPY, core_leaf)\n",
    "        np.save(CORE_LEAF_SOURCE_NPY, core_src)\n",
    "\n",
    "    return {i: int(core_leaf[i]) for i in range(n)}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3.5) Self-training to refine core leaf\n",
    "# =========================\n",
    "class PseudoLeafDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class LinearLeafClassifier(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def _compute_leaf_maxsim(review_emb_all: np.ndarray, leaf_emb: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    review별 leaf에 대한 max cosine(sim) (배치로 계산)\n",
    "    \"\"\"\n",
    "    leaf_t = leaf_emb.T\n",
    "    n = review_emb_all.shape[0]\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "    bs = 2048\n",
    "    for i in range(0, n, bs):\n",
    "        chunk = review_emb_all[i:i + bs]\n",
    "        sims = chunk @ leaf_t\n",
    "        out[i:i + bs] = sims.max(axis=1).astype(np.float32)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Self-training으로 core leaf 정제\n",
    "def self_train_refine_core_leaf(\n",
    "    leaf_ids: List[int],\n",
    "    leaf_emb: np.ndarray,\n",
    "    review_emb_all: np.ndarray,\n",
    "    core_leaf_map: Dict[int, int],\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    초기 core_leaf(cosine/LLM)를 seed로 삼아 leaf-classifier를 학습하고,\n",
    "    높은 confidence 예측을 pseudo label로 편입하는 self-training 수행.\n",
    "\n",
    "    반환:\n",
    "      - st_core_leaf: (N,) int32 (refine된 core leaf class_id)\n",
    "      - st_conf: (N,) float32 (해당 예측 confidence)\n",
    "    \"\"\"\n",
    "    n = review_emb_all.shape[0]\n",
    "    leaf_ids = list(leaf_ids)\n",
    "    leaf_id_to_index = {cid: i for i, cid in enumerate(leaf_ids)}\n",
    "\n",
    "    # 코어/소스 로드(기존 캐시 활용)\n",
    "    if os.path.exists(CORE_LEAF_NPY) and os.path.exists(CORE_LEAF_SOURCE_NPY) and os.path.exists(CORE_LEAF_META_JSON):\n",
    "        core_leaf = np.load(CORE_LEAF_NPY).astype(np.int32)\n",
    "        core_src = np.load(CORE_LEAF_SOURCE_NPY).astype(np.uint8)\n",
    "        meta = safe_json_load(CORE_LEAF_META_JSON, default={})\n",
    "        thr = float(meta.get(\"threshold\", THRESHOLD_FLOOR))\n",
    "    else:\n",
    "        # 비정상 케이스: map로 대체\n",
    "        core_leaf = np.array([core_leaf_map[i] for i in range(n)], dtype=np.int32)\n",
    "        core_src = np.zeros(n, dtype=np.uint8)\n",
    "        thr = THRESHOLD_FLOOR\n",
    "\n",
    "    # self-training 재개 체크\n",
    "    if os.path.exists(SELF_TRAIN_CORE_LEAF_NPY) and os.path.exists(SELF_TRAIN_CONF_NPY) and os.path.exists(SELF_TRAIN_META_JSON):\n",
    "        st_meta = safe_json_load(SELF_TRAIN_META_JSON, default={})\n",
    "        st_core_leaf = np.load(SELF_TRAIN_CORE_LEAF_NPY).astype(np.int32)\n",
    "        st_conf = np.load(SELF_TRAIN_CONF_NPY).astype(np.float32)\n",
    "        if len(st_core_leaf) == n and len(st_conf) == n and int(st_meta.get(\"done\", 0)) == 1:\n",
    "            print(\"[Self-training] Found completed cache. Skip training.\")\n",
    "            return st_core_leaf, st_conf\n",
    "\n",
    "    # seed 선정: (LLM 확정) OR (cosine maxsim >= thr)\n",
    "    leaf_maxsim = _compute_leaf_maxsim(review_emb_all, leaf_emb)\n",
    "    seed_thr = thr if SELF_TRAIN_SEED_MIN_SIM is None else float(SELF_TRAIN_SEED_MIN_SIM)\n",
    "    seed_mask = (core_src == 1) | (leaf_maxsim >= float(seed_thr))\n",
    "\n",
    "    seed_idx = np.where(seed_mask)[0]\n",
    "    if seed_idx.size == 0:\n",
    "        # fallback: 전부 사용(권장X, 그래도 진행)\n",
    "        seed_idx = np.arange(n)\n",
    "\n",
    "    # 현재 레이블 -> leaf index로 변환 (seed만)\n",
    "    seed_y = []\n",
    "    seed_x = []\n",
    "    for i in seed_idx.tolist():\n",
    "        cid = int(core_leaf[i])\n",
    "        if cid in leaf_id_to_index:\n",
    "            seed_x.append(review_emb_all[i])\n",
    "            seed_y.append(leaf_id_to_index[cid])\n",
    "    if len(seed_x) < 100:\n",
    "        print(\"[Self-training] Too few seed labels. Skip self-training and use base core_leaf.\")\n",
    "        st_core_leaf = core_leaf.copy().astype(np.int32)\n",
    "        st_conf = np.ones(n, dtype=np.float32) * 0.0\n",
    "        # base confidence: leaf_maxsim (정규화 아님)\n",
    "        st_conf = np.clip(leaf_maxsim, 0.0, 1.0).astype(np.float32)\n",
    "        np.save(SELF_TRAIN_CORE_LEAF_NPY, st_core_leaf)\n",
    "        np.save(SELF_TRAIN_CONF_NPY, st_conf)\n",
    "        safe_json_save({\"done\": 1, \"reason\": \"too_few_seed\"}, SELF_TRAIN_META_JSON)\n",
    "        return st_core_leaf, st_conf\n",
    "\n",
    "    seed_x = np.stack(seed_x, axis=0).astype(np.float32)\n",
    "    seed_y = np.array(seed_y, dtype=np.int64)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    in_dim = review_emb_all.shape[1]\n",
    "    num_leaf = len(leaf_ids)\n",
    "\n",
    "    model = LinearLeafClassifier(in_dim=in_dim, num_classes=num_leaf).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=SELF_TRAIN_LR, weight_decay=SELF_TRAIN_WEIGHT_DECAY)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    # self-training state\n",
    "    st_core_leaf = core_leaf.copy().astype(np.int32)\n",
    "    st_conf = np.zeros(n, dtype=np.float32)\n",
    "\n",
    "    for it in range(SELF_TRAIN_NUM_ITERS):\n",
    "        # (iter별) 학습 데이터 구성: 현재 st_core_leaf 기반에서 confidence 높은 것만\n",
    "        # 초기 iter에서는 seed만. 이후에는 st_conf가 충분히 높은 것들까지 포함.\n",
    "        if it == 0:\n",
    "            train_mask = seed_mask\n",
    "        else:\n",
    "            train_mask = seed_mask | (st_conf >= float(SELF_TRAIN_CONF_THRESHOLD))\n",
    "\n",
    "        train_idx = np.where(train_mask)[0]\n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        for i in train_idx.tolist():\n",
    "            cid = int(st_core_leaf[i])\n",
    "            if cid in leaf_id_to_index:\n",
    "                train_x.append(review_emb_all[i])\n",
    "                train_y.append(leaf_id_to_index[cid])\n",
    "\n",
    "        if len(train_x) < 100:\n",
    "            print(f\"[Self-training] Iter {it}: too few training samples ({len(train_x)}). Stop.\")\n",
    "            break\n",
    "\n",
    "        train_x = np.stack(train_x, axis=0).astype(np.float32)\n",
    "        train_y = np.array(train_y, dtype=np.int64)\n",
    "\n",
    "        ds = PseudoLeafDataset(train_x, train_y)\n",
    "        dl = DataLoader(ds, batch_size=SELF_TRAIN_BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "        model.train()\n",
    "        for ep in range(SELF_TRAIN_EPOCHS_PER_ITER):\n",
    "            total_loss = 0.0\n",
    "            for xb, yb in dl:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total_loss += float(loss.item()) * xb.size(0)\n",
    "            avg_loss = total_loss / max(len(ds), 1)\n",
    "            print(f\"[Self-training] iter={it} epoch={ep} loss={avg_loss:.4f} train_n={len(ds)}\")\n",
    "\n",
    "        # 예측 및 pseudo label 편입\n",
    "        model.eval()\n",
    "        new_candidates = []\n",
    "\n",
    "        bs = 4096\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n, bs):\n",
    "                xb = torch.from_numpy(review_emb_all[i:i + bs]).float().to(device)\n",
    "                logits = model(xb)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                conf, pred = torch.max(probs, dim=1)\n",
    "                conf = conf.detach().cpu().numpy().astype(np.float32)\n",
    "                pred = pred.detach().cpu().numpy().astype(np.int32)\n",
    "                for j in range(len(conf)):\n",
    "                    idx = i + j\n",
    "                    c = float(conf[j])\n",
    "                    if c >= float(SELF_TRAIN_CONF_THRESHOLD):\n",
    "                        new_cid = int(leaf_ids[int(pred[j])])\n",
    "                        new_candidates.append((c, idx, new_cid))\n",
    "\n",
    "        # 상위 confidence부터 최대 N개 편입\n",
    "        new_candidates.sort(key=lambda x: -x[0])\n",
    "        added = 0\n",
    "        for c, idx, new_cid in new_candidates:\n",
    "            # seed는 그대로 두되, seed가 아닌 경우에 주로 업데이트\n",
    "            if seed_mask[idx]:\n",
    "                # seed와 동일 라벨이면 conf만 갱신\n",
    "                if int(st_core_leaf[idx]) == int(new_cid):\n",
    "                    st_conf[idx] = max(st_conf[idx], float(c))\n",
    "                continue\n",
    "\n",
    "            # 업데이트\n",
    "            st_core_leaf[idx] = int(new_cid)\n",
    "            st_conf[idx] = max(st_conf[idx], float(c))\n",
    "            added += 1\n",
    "            if added >= int(SELF_TRAIN_MAX_NEW_PER_ITER):\n",
    "                break\n",
    "\n",
    "        print(f\"[Self-training] iter={it}: added_new={added}, total_highconf={(st_conf>=SELF_TRAIN_CONF_THRESHOLD).sum()}\")\n",
    "\n",
    "        # iter checkpoint\n",
    "        np.save(SELF_TRAIN_CORE_LEAF_NPY, st_core_leaf)\n",
    "        np.save(SELF_TRAIN_CONF_NPY, st_conf)\n",
    "        safe_json_save({\"done\": 0, \"iter\": it, \"added\": added}, SELF_TRAIN_META_JSON)\n",
    "\n",
    "    # 완료 체크포인트\n",
    "    np.save(SELF_TRAIN_CORE_LEAF_NPY, st_core_leaf)\n",
    "    np.save(SELF_TRAIN_CONF_NPY, st_conf)\n",
    "    safe_json_save({\"done\": 1}, SELF_TRAIN_META_JSON)\n",
    "    return st_core_leaf, st_conf\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Greedy parent selection (max 3 labels) - DAG 대응\n",
    "# =========================\n",
    "def select_hierarchical_labels(\n",
    "    core_leaf_map: Dict[int, int],\n",
    "    review_emb_all: np.ndarray,\n",
    "    class_emb_all: np.ndarray,\n",
    "    parent_of: Dict[int, set],\n",
    "    all_rows: List[dict],\n",
    "    max_labels: int = 3\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    DAG에서 parent_of[child]가 set(parents) 이므로,\n",
    "    각 단계에서 '가장 유사한 parent'를 선택하는 방식으로 chain을 구성.\n",
    "    (leaf, best-parent, best-grandparent) 최대 3개.\n",
    "    \"\"\"\n",
    "    pid_to_labels = {}\n",
    "    for idx, r in enumerate(tqdm(all_rows, desc=\"Select hierarchical labels (DAG)\")):\n",
    "        pid = r[\"pid\"]\n",
    "        core = int(core_leaf_map[idx])\n",
    "        labels = [core]\n",
    "\n",
    "        cur = core\n",
    "        prev_sim = float(review_emb_all[idx] @ class_emb_all[cur])\n",
    "\n",
    "        for _ in range(max_labels - 1):\n",
    "            parents = list(parent_of.get(cur, set()))\n",
    "            if not parents:\n",
    "                break\n",
    "\n",
    "            # parent 후보 중 best\n",
    "            best_p = None\n",
    "            best_sim = -1.0\n",
    "            for p in parents:\n",
    "                s = float(review_emb_all[idx] @ class_emb_all[int(p)])\n",
    "                if s > best_sim:\n",
    "                    best_sim = s\n",
    "                    best_p = int(p)\n",
    "\n",
    "            if best_p is None:\n",
    "                break\n",
    "\n",
    "            # 중단 규칙(기존 변수명 유지)\n",
    "            if best_sim < float(PARENT_MIN_SIM):\n",
    "                break\n",
    "            abs_drop = float(prev_sim - best_sim)\n",
    "            rel_drop = abs_drop / max(float(prev_sim), 1e-9)\n",
    "            if abs_drop >= float(DROP_ABS_DELTA):\n",
    "                break\n",
    "            if rel_drop >= float(DROP_REL_RATIO):\n",
    "                break\n",
    "\n",
    "            labels.append(int(best_p))\n",
    "            cur = int(best_p)\n",
    "            prev_sim = float(best_sim)\n",
    "\n",
    "        pid_to_labels[pid] = labels\n",
    "    return pid_to_labels\n",
    "\n",
    "\n",
    "def write_submission(test_rows, all_rows, review_emb_all, leaf_ids, leaf_emb, pid_to_labels, out_path):\n",
    "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"id\", \"labels\"])\n",
    "        for r in test_rows:\n",
    "            pid = r[\"pid\"]\n",
    "            labels = pid_to_labels.get(pid, [])\n",
    "            if not labels:\n",
    "                # fallback: 최소 1개 라벨\n",
    "                idx = None\n",
    "                for i, rr in enumerate(all_rows):\n",
    "                    if rr.get(\"split\") == \"test\" and rr.get(\"pid\") == pid:\n",
    "                        idx = i\n",
    "                        break\n",
    "                if idx is not None:\n",
    "                    sims = review_emb_all[idx] @ leaf_emb.T\n",
    "                    cid = leaf_ids[int(np.argmax(sims))]\n",
    "                    labels = [int(cid)]\n",
    "            w.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    set_seed(42)\n",
    "\n",
    "    # 0) Load data\n",
    "    id2name, _ = load_classes(CLASSES_PATH)\n",
    "    num_classes = len(id2name)\n",
    "    cid2kws = load_keywords(KW_PATH)\n",
    "\n",
    "    # 1) DAG hierarchy (다중 parent)\n",
    "    parent_of, children_of = load_hierarchy(HIER_PATH, num_classes=num_classes)\n",
    "    leaf_ids = get_leaf_nodes(children_of, num_classes=num_classes)\n",
    "\n",
    "    print(f\"[Hierarchy] num_classes={num_classes}, leaves={len(leaf_ids)}\")\n",
    "\n",
    "    # 2) Class descriptions (cached)\n",
    "    class_desc_map = generate_class_descriptions(id2name, cid2kws)\n",
    "\n",
    "    # 3) Corpus\n",
    "    train_rows = load_corpus_any(TRAIN_CORPUS_PATH, \"train\")\n",
    "    test_rows = load_corpus_any(TEST_CORPUS_PATH, \"test\")\n",
    "    all_rows = train_rows + test_rows\n",
    "\n",
    "    # 4) Embeddings (cached)\n",
    "    class_emb_all, review_emb_all = build_or_load_embeddings(class_desc_map, all_rows, num_classes)\n",
    "\n",
    "    # 5) Leaf embeddings\n",
    "    leaf_emb = class_emb_all[np.array(leaf_ids, dtype=np.int32)]\n",
    "\n",
    "    # 6) Core leaf inference (cosine + LLM fallback, checkpoint)\n",
    "    core_leaf_map = infer_core_leaf_classes(\n",
    "        leaf_ids=leaf_ids,\n",
    "        leaf_emb=leaf_emb,\n",
    "        review_emb_all=review_emb_all,\n",
    "        all_reviews=all_rows,\n",
    "        id2name=id2name,\n",
    "        class_desc_map=class_desc_map,\n",
    "    )\n",
    "\n",
    "    # 7) Self-training refine (checkpoint)\n",
    "    st_core_leaf, st_conf = self_train_refine_core_leaf(\n",
    "        leaf_ids=leaf_ids,\n",
    "        leaf_emb=leaf_emb,\n",
    "        review_emb_all=review_emb_all,\n",
    "        core_leaf_map=core_leaf_map,\n",
    "    )\n",
    "    # 최종 core_leaf_map 갱신(인덱스 -> class_id)\n",
    "    core_leaf_map = {i: int(st_core_leaf[i]) for i in range(len(st_core_leaf))}\n",
    "\n",
    "    # 8) Hierarchical label selection (DAG)\n",
    "    pid_to_labels = select_hierarchical_labels(\n",
    "        core_leaf_map=core_leaf_map,\n",
    "        review_emb_all=review_emb_all,\n",
    "        class_emb_all=class_emb_all,\n",
    "        parent_of=parent_of,\n",
    "        all_rows=all_rows,\n",
    "        max_labels=3,\n",
    "    )\n",
    "\n",
    "    # 9) Write submission\n",
    "    write_submission(\n",
    "        test_rows=test_rows,\n",
    "        all_rows=all_rows,\n",
    "        review_emb_all=review_emb_all,\n",
    "        leaf_ids=leaf_ids,\n",
    "        leaf_emb=leaf_emb,\n",
    "        pid_to_labels=pid_to_labels,\n",
    "        out_path=SUBMISSION_PATH,\n",
    "    )\n",
    "\n",
    "    print(f\"[Done] wrote {SUBMISSION_PATH}\")\n",
    "    print(f\"[API calls] {API_CALL_COUNT}/{API_CALL_LIMIT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
